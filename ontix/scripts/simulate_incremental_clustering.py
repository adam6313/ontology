#!/usr/bin/env python3
"""
æ¨¡æ“¬å¢é‡ Cluster åˆ†é…æµç¨‹

å±•ç¤ºï¼š
1. åˆå§‹åŒ– - ç”¨éƒ¨åˆ†è³‡æ–™å»ºç«‹ clusters
2. å³æ™‚åˆ†é… - KNN åˆ†é…æ–°æ–‡ç« 
3. Pending è™•ç† - Micro-batch HDBSCAN
4. çµ±è¨ˆåˆ†æ
"""
import numpy as np
import psycopg2
from psycopg2.extras import RealDictCursor
from hdbscan import HDBSCAN
from umap import UMAP
from sklearn.metrics.pairwise import cosine_similarity
from collections import defaultdict
import time
import random

# ============================================================
# é…ç½®
# ============================================================
CONFIG = {
    "confidence_threshold": 0.6,      # ä¿¡å¿ƒåº¦é–¾å€¼
    "pending_batch_size": 100,        # Micro-batch å¤§å°
    "min_cluster_size": 3,            # HDBSCAN åƒæ•¸
    "initial_ratio": 0.7,             # åˆå§‹åŒ–ç”¨çš„è³‡æ–™æ¯”ä¾‹
}

# ============================================================
# è³‡æ–™åº«é€£ç·š
# ============================================================
conn = psycopg2.connect(
    host="localhost", port=5432, dbname="ontix_dev",
    user="ontix", password="ontix_dev"
)

def parse_embedding(emb_str):
    if emb_str is None:
        return None
    if isinstance(emb_str, (list, np.ndarray)):
        return np.array(emb_str)
    clean = emb_str.strip('[]')
    return np.array([float(x) for x in clean.split(',')])

# ============================================================
# Centroid Cache (æ¨¡æ“¬ Redis)
# ============================================================
class CentroidCache:
    """æ¨¡æ“¬ Redis centroid cache"""

    def __init__(self):
        self.centroids = {}  # cluster_id -> embedding
        self.cluster_sizes = {}  # cluster_id -> count
        self.cluster_names = {}  # cluster_id -> name

    def add_cluster(self, cluster_id, centroid, size, name=""):
        self.centroids[cluster_id] = centroid
        self.cluster_sizes[cluster_id] = size
        self.cluster_names[cluster_id] = name

    def get_all_centroids(self):
        return self.centroids

    def update_centroid(self, cluster_id, new_centroid, new_size):
        self.centroids[cluster_id] = new_centroid
        self.cluster_sizes[cluster_id] = new_size

    def stats(self):
        return {
            "total_clusters": len(self.centroids),
            "total_posts": sum(self.cluster_sizes.values()),
            "sizes": dict(self.cluster_sizes)
        }

# ============================================================
# Pending Queue (æ¨¡æ“¬ Redis sorted set)
# ============================================================
class PendingQueue:
    """æ¨¡æ“¬ pending queue"""

    def __init__(self):
        self.queue = []  # [(post_id, embedding, content)]

    def add(self, post_id, embedding, content):
        self.queue.append((post_id, embedding, content))

    def pop_batch(self, size):
        batch = self.queue[:size]
        self.queue = self.queue[size:]
        return batch

    def size(self):
        return len(self.queue)

# ============================================================
# å³æ™‚åˆ†é…å™¨
# ============================================================
class RealtimeAssigner:
    """å³æ™‚ KNN åˆ†é…"""

    def __init__(self, centroid_cache, confidence_threshold=0.6):
        self.cache = centroid_cache
        self.threshold = confidence_threshold
        self.stats = {
            "assigned": 0,
            "pending": 0,
            "total_latency_ms": 0
        }

    def assign(self, post_id, embedding):
        """åˆ†é…å–®ç¯‡æ–‡ç« ï¼Œè¿”å› (cluster_id, confidence, is_pending)"""
        start = time.time()

        centroids = self.cache.get_all_centroids()
        if not centroids:
            return None, 0, True

        # è¨ˆç®—èˆ‡æ‰€æœ‰ centroid çš„ç›¸ä¼¼åº¦
        centroid_ids = list(centroids.keys())
        centroid_vectors = np.array([centroids[cid] for cid in centroid_ids])

        similarities = cosine_similarity([embedding], centroid_vectors)[0]

        # æ‰¾æœ€é«˜çš„å…©å€‹
        sorted_indices = np.argsort(similarities)[::-1]
        best_idx = sorted_indices[0]
        second_idx = sorted_indices[1] if len(sorted_indices) > 1 else sorted_indices[0]

        best_cluster = centroid_ids[best_idx]
        best_sim = similarities[best_idx]
        second_sim = similarities[second_idx]

        # ä¿¡å¿ƒåº¦ = top1 ç›¸ä¼¼åº¦ * (top1 - top2 çš„å·®è·)
        gap = best_sim - second_sim
        confidence = best_sim * (1 + gap)  # ç°¡åŒ–çš„ä¿¡å¿ƒåº¦å…¬å¼

        latency_ms = (time.time() - start) * 1000
        self.stats["total_latency_ms"] += latency_ms

        is_pending = confidence < self.threshold
        if is_pending:
            self.stats["pending"] += 1
        else:
            self.stats["assigned"] += 1

        return best_cluster, confidence, is_pending

    def get_stats(self):
        total = self.stats["assigned"] + self.stats["pending"]
        avg_latency = self.stats["total_latency_ms"] / total if total > 0 else 0
        return {
            **self.stats,
            "total": total,
            "pending_rate": self.stats["pending"] / total if total > 0 else 0,
            "avg_latency_ms": avg_latency
        }

# ============================================================
# Micro-batch è™•ç†å™¨
# ============================================================
class MicroBatchProcessor:
    """è™•ç† pending queue çš„ HDBSCAN"""

    def __init__(self, centroid_cache, min_cluster_size=3):
        self.cache = centroid_cache
        self.min_cluster_size = min_cluster_size
        self.stats = {
            "batches_processed": 0,
            "posts_assigned": 0,
            "new_clusters_created": 0,
            "outliers": 0
        }

    def process(self, pending_batch, existing_next_cluster_id):
        """è™•ç†ä¸€æ‰¹ pending æ–‡ç« """
        if len(pending_batch) < self.min_cluster_size:
            # å¤ªå°‘ï¼Œå…¨éƒ¨æ¨™è¨˜ç‚º outlier
            self.stats["outliers"] += len(pending_batch)
            return [], existing_next_cluster_id

        # æå– embeddings
        embeddings = np.array([item[1] for item in pending_batch])

        # å˜—è©¦ç”¨ HDBSCAN èšé¡
        # å…ˆé™ç¶­ï¼ˆå¦‚æœè³‡æ–™å¤ å¤šï¼‰
        if len(embeddings) > 10:
            n_neighbors = min(5, len(embeddings) - 1)
            n_components = min(10, len(embeddings) - 2)
            umap = UMAP(n_neighbors=n_neighbors, n_components=n_components,
                       min_dist=0.0, metric='cosine', random_state=42)
            reduced = umap.fit_transform(embeddings)
        else:
            reduced = embeddings

        hdbscan = HDBSCAN(
            min_cluster_size=self.min_cluster_size,
            min_samples=1,
            metric='euclidean'
        )
        labels = hdbscan.fit_predict(reduced)

        # è™•ç†çµæœ
        assignments = []
        new_clusters = defaultdict(list)

        for i, label in enumerate(labels):
            post_id, embedding, content = pending_batch[i]

            if label == -1:
                # Outlier - å˜—è©¦åˆ†é…åˆ°ç¾æœ‰æœ€è¿‘çš„ cluster
                centroids = self.cache.get_all_centroids()
                if centroids:
                    centroid_ids = list(centroids.keys())
                    centroid_vectors = np.array([centroids[cid] for cid in centroid_ids])
                    sims = cosine_similarity([embedding], centroid_vectors)[0]
                    best_idx = np.argmax(sims)
                    assignments.append((post_id, centroid_ids[best_idx], sims[best_idx]))
                    self.stats["posts_assigned"] += 1
                else:
                    self.stats["outliers"] += 1
            else:
                # å±¬æ–¼æ–°ç™¼ç¾çš„ cluster
                new_clusters[label].append((post_id, embedding, content))

        # å‰µå»ºæ–° cluster
        next_id = existing_next_cluster_id
        for label, members in new_clusters.items():
            if len(members) >= self.min_cluster_size:
                # è¨ˆç®— centroid
                member_embeddings = np.array([m[1] for m in members])
                centroid = member_embeddings.mean(axis=0)

                # åŠ å…¥ cache
                self.cache.add_cluster(next_id, centroid, len(members), f"æ–°ç¾¤çµ„_{next_id}")

                # åˆ†é…æˆå“¡
                for post_id, _, _ in members:
                    assignments.append((post_id, next_id, 0.9))

                self.stats["new_clusters_created"] += 1
                self.stats["posts_assigned"] += len(members)
                next_id += 1
            else:
                # å¤ªå°ï¼Œæ¨™è¨˜ç‚º outlier
                self.stats["outliers"] += len(members)

        self.stats["batches_processed"] += 1
        return assignments, next_id

    def get_stats(self):
        return self.stats

# ============================================================
# ä¸»æ¨¡æ“¬æµç¨‹
# ============================================================
def main():
    print("=" * 70)
    print("å¢é‡ Cluster åˆ†é…æ¨¡æ“¬")
    print("=" * 70)

    # 1. è®€å–è³‡æ–™
    print("\nğŸ“Š è¼‰å…¥è³‡æ–™...")
    with conn.cursor(cursor_factory=RealDictCursor) as cur:
        cur.execute("""
            SELECT p.post_id, p.content, pe.embedding
            FROM posts p
            JOIN post_embeddings pe ON p.post_id = pe.post_id
        """)
        posts = cur.fetchall()

    all_posts = []
    for p in posts:
        emb = parse_embedding(p['embedding'])
        if emb is not None:
            all_posts.append({
                'post_id': p['post_id'],
                'content': p['content'][:200] if p['content'] else "",
                'embedding': emb
            })

    print(f"   è¼‰å…¥ {len(all_posts)} ç¯‡è²¼æ–‡")

    if len(all_posts) < 20:
        print("âŒ è³‡æ–™å¤ªå°‘ï¼Œè«‹å…ˆæ¨é€æ›´å¤šè³‡æ–™")
        return

    # 2. åˆ†å‰²è³‡æ–™ï¼šåˆå§‹åŒ–ç”¨ vs æ¨¡æ“¬æ–°é€²ç”¨
    random.shuffle(all_posts)
    split_idx = int(len(all_posts) * CONFIG["initial_ratio"])
    initial_posts = all_posts[:split_idx]
    new_posts = all_posts[split_idx:]

    print(f"   åˆå§‹åŒ–è³‡æ–™: {len(initial_posts)} ç¯‡")
    print(f"   æ¨¡æ“¬æ–°é€²è³‡æ–™: {len(new_posts)} ç¯‡")

    # 3. åˆå§‹åŒ– - ç”¨ HDBSCAN å»ºç«‹åˆå§‹ clusters
    print("\nğŸ”§ åˆå§‹åŒ– Clusters (HDBSCAN)...")

    embeddings = np.array([p['embedding'] for p in initial_posts])

    # UMAP é™ç¶­
    n_neighbors = min(15, len(embeddings) - 1)
    n_components = min(20, len(embeddings) - 2)
    umap = UMAP(n_neighbors=n_neighbors, n_components=n_components,
                min_dist=0.0, metric='cosine', random_state=42)
    reduced = umap.fit_transform(embeddings)

    # HDBSCAN
    hdbscan = HDBSCAN(
        min_cluster_size=CONFIG["min_cluster_size"],
        min_samples=max(1, CONFIG["min_cluster_size"] // 2),
        metric='euclidean'
    )
    labels = hdbscan.fit_predict(reduced)

    # å»ºç«‹ centroid cache
    cache = CentroidCache()
    cluster_members = defaultdict(list)

    for i, label in enumerate(labels):
        if label != -1:
            cluster_members[label].append(i)

    for cluster_id, members in cluster_members.items():
        member_embeddings = embeddings[members]
        centroid = member_embeddings.mean(axis=0)
        cache.add_cluster(cluster_id, centroid, len(members), f"ç¾¤çµ„_{cluster_id}")

    initial_stats = cache.stats()
    print(f"   å»ºç«‹ {initial_stats['total_clusters']} å€‹ clusters")
    print(f"   è¦†è“‹ {initial_stats['total_posts']}/{len(initial_posts)} ç¯‡ ({initial_stats['total_posts']/len(initial_posts)*100:.1f}%)")

    # 4. æ¨¡æ“¬å³æ™‚åˆ†é…
    print("\nâš¡ æ¨¡æ“¬å³æ™‚åˆ†é…...")
    print(f"   ä¿¡å¿ƒåº¦é–¾å€¼: {CONFIG['confidence_threshold']}")

    assigner = RealtimeAssigner(cache, CONFIG["confidence_threshold"])
    pending_queue = PendingQueue()
    assignments = []

    for post in new_posts:
        cluster_id, confidence, is_pending = assigner.assign(
            post['post_id'],
            post['embedding']
        )

        if is_pending:
            pending_queue.add(post['post_id'], post['embedding'], post['content'])
        else:
            assignments.append((post['post_id'], cluster_id, confidence))

    realtime_stats = assigner.get_stats()
    print(f"   è™•ç† {realtime_stats['total']} ç¯‡")
    print(f"   ç›´æ¥åˆ†é…: {realtime_stats['assigned']} ({100-realtime_stats['pending_rate']*100:.1f}%)")
    print(f"   é€²å…¥ pending: {realtime_stats['pending']} ({realtime_stats['pending_rate']*100:.1f}%)")
    print(f"   å¹³å‡å»¶é²: {realtime_stats['avg_latency_ms']:.2f} ms")

    # 5. æ¨¡æ“¬ Micro-batch è™•ç†
    print("\nğŸ”„ æ¨¡æ“¬ Micro-batch è™•ç†...")
    print(f"   Pending queue å¤§å°: {pending_queue.size()}")

    processor = MicroBatchProcessor(cache, CONFIG["min_cluster_size"])
    next_cluster_id = max(cache.get_all_centroids().keys()) + 1 if cache.get_all_centroids() else 0

    batch_num = 0
    while pending_queue.size() > 0:
        batch = pending_queue.pop_batch(CONFIG["pending_batch_size"])
        batch_assignments, next_cluster_id = processor.process(batch, next_cluster_id)
        assignments.extend(batch_assignments)
        batch_num += 1
        print(f"   Batch {batch_num}: è™•ç† {len(batch)} ç¯‡")

    batch_stats = processor.get_stats()
    print(f"\n   Micro-batch çµ±è¨ˆ:")
    print(f"   - æ‰¹æ¬¡æ•¸: {batch_stats['batches_processed']}")
    print(f"   - åˆ†é…æˆåŠŸ: {batch_stats['posts_assigned']}")
    print(f"   - æ–°å»º cluster: {batch_stats['new_clusters_created']}")
    print(f"   - Outliers: {batch_stats['outliers']}")

    # 6. æœ€çµ‚çµ±è¨ˆ
    print("\n" + "=" * 70)
    print("ğŸ“ˆ æœ€çµ‚çµ±è¨ˆ")
    print("=" * 70)

    final_stats = cache.stats()
    print(f"\nCluster ç‹€æ…‹:")
    print(f"   ç¸½ clusters: {final_stats['total_clusters']}")
    print(f"   ç¸½è¦†è“‹è²¼æ–‡: {final_stats['total_posts']}")

    # Cluster å¤§å°åˆ†ä½ˆ
    sizes = list(final_stats['sizes'].values())
    print(f"\nCluster å¤§å°åˆ†ä½ˆ:")
    print(f"   æœ€å°: {min(sizes)}")
    print(f"   æœ€å¤§: {max(sizes)}")
    print(f"   å¹³å‡: {np.mean(sizes):.1f}")
    print(f"   ä¸­ä½æ•¸: {np.median(sizes):.1f}")

    # åˆ†é…çµ±è¨ˆ
    total_new = len(new_posts)
    assigned_count = len(assignments)
    coverage = assigned_count / total_new * 100 if total_new > 0 else 0

    print(f"\næ–°æ–‡ç« åˆ†é…:")
    print(f"   æ–°æ–‡ç« ç¸½æ•¸: {total_new}")
    print(f"   æˆåŠŸåˆ†é…: {assigned_count} ({coverage:.1f}%)")

    # ä¿¡å¿ƒåº¦åˆ†ä½ˆ
    if assignments:
        confidences = [a[2] for a in assignments]
        print(f"\nä¿¡å¿ƒåº¦åˆ†ä½ˆ:")
        print(f"   æœ€ä½: {min(confidences):.3f}")
        print(f"   æœ€é«˜: {max(confidences):.3f}")
        print(f"   å¹³å‡: {np.mean(confidences):.3f}")

    print("\n" + "=" * 70)
    print("âœ… æ¨¡æ“¬å®Œæˆ")
    print("=" * 70)

    # 7. æµç¨‹ç¸½çµ
    print("\nğŸ“ æµç¨‹ç¸½çµ:")
    print("""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  1. åˆå§‹åŒ–                                                   â”‚
    â”‚     â””â”€ HDBSCAN å»ºç«‹åˆå§‹ clusters + centroids                â”‚
    â”‚                                                             â”‚
    â”‚  2. å³æ™‚åˆ†é… (æ¯ç¯‡æ–°æ–‡ç« )                                    â”‚
    â”‚     â””â”€ KNN æ‰¾æœ€è¿‘ centroid                                  â”‚
    â”‚     â””â”€ é«˜ä¿¡å¿ƒåº¦ â†’ ç›´æ¥åˆ†é…                                   â”‚
    â”‚     â””â”€ ä½ä¿¡å¿ƒåº¦ â†’ pending queue                             â”‚
    â”‚                                                             â”‚
    â”‚  3. Micro-batch (å®šæœŸè™•ç† pending)                          â”‚
    â”‚     â””â”€ å°è¦æ¨¡ HDBSCAN                                       â”‚
    â”‚     â””â”€ ç™¼ç¾æ–° cluster æˆ–åˆ†é…åˆ°ç¾æœ‰                           â”‚
    â”‚                                                             â”‚
    â”‚  4. æ¡æ¨£ç¶­è­· (æ¯é€±ï¼Œæœ¬æ¬¡æœªæ¨¡æ“¬)                               â”‚
    â”‚     â””â”€ æ¡æ¨£é‡ç®— centroid                                    â”‚
    â”‚     â””â”€ åˆä½µ/æ¸…ç† clusters                                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)

if __name__ == "__main__":
    main()
    conn.close()
