#!/usr/bin/env python3
"""
ğŸ“Š Data Scientist å®Œæ•´å¯¦é©—å ±å‘Š
ç¤¾ç¾¤è²¼æ–‡åˆ†é¡ç³»çµ±åˆ†æ
"""

import numpy as np
import psycopg2
from psycopg2.extras import RealDictCursor
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')
import matplotlib.font_manager as fm
import seaborn as sns
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
from collections import Counter
import json
import yaml
from openai import OpenAI
import re
from datetime import datetime

# è¨­å®šä¸­æ–‡å­—é«”
chinese_fonts = ['/System/Library/Fonts/PingFang.ttc', '/System/Library/Fonts/STHeiti Light.ttc']
for font_path in chinese_fonts:
    import os
    if os.path.exists(font_path):
        fm.fontManager.addfont(font_path)
        matplotlib.rcParams['font.family'] = fm.FontProperties(fname=font_path).get_name()
        break
matplotlib.rcParams['axes.unicode_minus'] = False

# é€£ç·šè¨­å®š
conn = psycopg2.connect(
    host="localhost", port=5432, dbname="ontix_dev",
    user="ontix", password="ontix_dev"
)

with open('/Users/adam/poc/ontology/ontix/config/dev.yaml') as f:
    config = yaml.safe_load(f)
client = OpenAI(api_key=config['openai_api_key'])

OUTPUT_DIR = '/Users/adam/poc/ontology/ontix/scripts/experiment_results'
os.makedirs(OUTPUT_DIR, exist_ok=True)

def parse_embedding(emb_str):
    if emb_str is None:
        return None
    if isinstance(emb_str, (list, np.ndarray)):
        return np.array(emb_str)
    clean = emb_str.strip('[]')
    return np.array([float(x) for x in clean.split(',')])

# ============================================================
# å¯¦é©— 1: ä¸»é¡Œ Embedding é‡ç–Šåˆ†æ
# ============================================================
def experiment_1_topic_overlap():
    print("\n" + "="*70)
    print("ğŸ“Š å¯¦é©— 1: ä¸»é¡Œ Embedding é‡ç–Šåˆ†æ")
    print("="*70)

    with conn.cursor(cursor_factory=RealDictCursor) as cur:
        cur.execute("SELECT id, code, name, embedding FROM topics WHERE is_active = true AND embedding IS NOT NULL")
        topics = cur.fetchall()

    names = []
    embeddings = []
    for t in topics:
        emb = parse_embedding(t['embedding'])
        if emb is not None and len(emb) > 0:
            names.append(t['name'])
            embeddings.append(emb)

    emb_matrix = np.vstack(embeddings)
    sim_matrix = cosine_similarity(emb_matrix)

    # çµ±è¨ˆ
    upper_tri = sim_matrix[np.triu_indices(len(names), k=1)]

    results = {
        'n_topics': len(names),
        'similarity_mean': float(upper_tri.mean()),
        'similarity_std': float(upper_tri.std()),
        'similarity_max': float(upper_tri.max()),
        'similarity_min': float(upper_tri.min()),
        'high_sim_pairs': []
    }

    # æ‰¾é«˜ç›¸ä¼¼å°
    for i in range(len(names)):
        for j in range(i+1, len(names)):
            if sim_matrix[i][j] > 0.40:
                results['high_sim_pairs'].append({
                    'topic1': names[i],
                    'topic2': names[j],
                    'similarity': float(sim_matrix[i][j])
                })

    results['high_sim_pairs'].sort(key=lambda x: -x['similarity'])

    print(f"\nğŸ“ˆ ä¸»é¡Œæ•¸é‡: {results['n_topics']}")
    print(f"ğŸ“ˆ ç›¸ä¼¼åº¦çµ±è¨ˆ:")
    print(f"   å¹³å‡: {results['similarity_mean']:.4f}")
    print(f"   æ¨™æº–å·®: {results['similarity_std']:.4f}")
    print(f"   æœ€é«˜: {results['similarity_max']:.4f} ")
    print(f"   æœ€ä½: {results['similarity_min']:.4f}")
    print(f"\nâš ï¸  ç›¸å°é«˜ç›¸ä¼¼å° (>0.40): {len(results['high_sim_pairs'])} å°")
    for p in results['high_sim_pairs'][:5]:
        print(f"   {p['topic1']} â†” {p['topic2']}: {p['similarity']:.3f}")

    # çµè«–
    results['conclusion'] = "ä¸»é¡Œå®šç¾©åˆ†é›¢åº¦è‰¯å¥½" if results['similarity_max'] < 0.6 else "å­˜åœ¨é«˜åº¦é‡ç–Šä¸»é¡Œ"
    print(f"\nâœ… çµè«–: {results['conclusion']}")

    return results, sim_matrix, names

# ============================================================
# å¯¦é©— 2: åˆ†é¡æ¸…æ™°åº¦åˆ†æ
# ============================================================
def experiment_2_classification_clarity():
    print("\n" + "="*70)
    print("ğŸ“Š å¯¦é©— 2: åˆ†é¡æ¸…æ™°åº¦åˆ†æ (Gap Distribution)")
    print("="*70)

    with conn.cursor(cursor_factory=RealDictCursor) as cur:
        cur.execute("""
            SELECT
                p.post_id,
                p.content,
                LENGTH(p.content) as content_length,
                t1.name as top1_topic,
                s1.similarity as top1_sim,
                t2.name as top2_topic,
                s2.similarity as top2_sim,
                (s1.similarity - s2.similarity) as gap
            FROM posts p
            JOIN post_topic_scores s1 ON p.post_id::text = s1.post_id::text AND s1.rank = 1
            JOIN post_topic_scores s2 ON p.post_id::text = s2.post_id::text AND s2.rank = 2
            JOIN topics t1 ON s1.topic_id = t1.id
            JOIN topics t2 ON s2.topic_id = t2.id
        """)
        posts = cur.fetchall()

    gaps = [p['gap'] for p in posts]
    top1_sims = [p['top1_sim'] for p in posts]
    content_lengths = [p['content_length'] for p in posts]

    # åˆ†é¡æ¸…æ™°åº¦åˆ†ä½ˆ
    ambiguous = sum(1 for g in gaps if g < 0.02)
    moderate = sum(1 for g in gaps if 0.02 <= g < 0.05)
    clear = sum(1 for g in gaps if g >= 0.05)
    total = len(gaps)

    results = {
        'total_posts': total,
        'ambiguous': {'count': ambiguous, 'pct': ambiguous/total*100},
        'moderate': {'count': moderate, 'pct': moderate/total*100},
        'clear': {'count': clear, 'pct': clear/total*100},
        'gap_stats': {
            'mean': float(np.mean(gaps)),
            'std': float(np.std(gaps)),
            'median': float(np.median(gaps)),
            'p25': float(np.percentile(gaps, 25)),
            'p75': float(np.percentile(gaps, 75))
        },
        'top1_sim_stats': {
            'mean': float(np.mean(top1_sims)),
            'std': float(np.std(top1_sims)),
            'min': float(np.min(top1_sims)),
            'max': float(np.max(top1_sims))
        }
    }

    print(f"\nğŸ“ˆ ç¸½è²¼æ–‡æ•¸: {total}")
    print(f"\nğŸ“Š åˆ†é¡æ¸…æ™°åº¦åˆ†ä½ˆ:")
    print(f"   ğŸ”´ æ¨¡ç³Š (gap < 0.02):    {ambiguous:3d} ({ambiguous/total*100:5.1f}%)")
    print(f"   ğŸŸ¡ ä¸­ç­‰ (0.02-0.05):     {moderate:3d} ({moderate/total*100:5.1f}%)")
    print(f"   ğŸŸ¢ æ¸…æ™° (gap >= 0.05):   {clear:3d} ({clear/total*100:5.1f}%)")
    print(f"\nğŸ“ˆ Gap çµ±è¨ˆ:")
    print(f"   å¹³å‡: {results['gap_stats']['mean']:.4f}")
    print(f"   ä¸­ä½æ•¸: {results['gap_stats']['median']:.4f}")
    print(f"   P25-P75: {results['gap_stats']['p25']:.4f} - {results['gap_stats']['p75']:.4f}")
    print(f"\nğŸ“ˆ Top1 ç›¸ä¼¼åº¦çµ±è¨ˆ:")
    print(f"   å¹³å‡: {results['top1_sim_stats']['mean']:.4f}")
    print(f"   ç¯„åœ: {results['top1_sim_stats']['min']:.4f} - {results['top1_sim_stats']['max']:.4f}")

    # ç¹ªè£½åˆ†ä½ˆåœ–
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    # Gap åˆ†ä½ˆ
    axes[0,0].hist(gaps, bins=30, edgecolor='black', alpha=0.7)
    axes[0,0].axvline(0.02, color='r', linestyle='--', label='Ambiguous threshold')
    axes[0,0].axvline(0.05, color='orange', linestyle='--', label='Clear threshold')
    axes[0,0].set_xlabel('Gap (Top1 - Top2 Similarity)')
    axes[0,0].set_ylabel('Count')
    axes[0,0].set_title('Gap Distribution')
    axes[0,0].legend()

    # Top1 ç›¸ä¼¼åº¦åˆ†ä½ˆ
    axes[0,1].hist(top1_sims, bins=30, edgecolor='black', alpha=0.7, color='green')
    axes[0,1].set_xlabel('Top1 Similarity')
    axes[0,1].set_ylabel('Count')
    axes[0,1].set_title('Top1 Similarity Distribution')

    # å…§å®¹é•·åº¦ vs Gap
    axes[1,0].scatter(content_lengths, gaps, alpha=0.5)
    axes[1,0].set_xlabel('Content Length (chars)')
    axes[1,0].set_ylabel('Gap')
    axes[1,0].set_title('Content Length vs Classification Gap')

    # æ¸…æ™°åº¦ pie chart
    labels = ['Ambiguous\n(gap<0.02)', 'Moderate\n(0.02-0.05)', 'Clear\n(gap>=0.05)']
    sizes = [ambiguous, moderate, clear]
    colors = ['#ff6b6b', '#ffd93d', '#6bcb77']
    axes[1,1].pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
    axes[1,1].set_title('Classification Clarity Distribution')

    plt.tight_layout()
    plt.savefig(f'{OUTPUT_DIR}/exp2_clarity_analysis.png', dpi=150)
    print(f"\nğŸ“Š åœ–è¡¨å·²å„²å­˜: {OUTPUT_DIR}/exp2_clarity_analysis.png")

    return results, posts

# ============================================================
# å¯¦é©— 3: å…§å®¹ç‰¹å¾µåˆ†æ
# ============================================================
def experiment_3_content_features(posts):
    print("\n" + "="*70)
    print("ğŸ“Š å¯¦é©— 3: å…§å®¹ç‰¹å¾µèˆ‡åˆ†é¡å“è³ªé—œä¿‚")
    print("="*70)

    def count_hashtags(text):
        return len(re.findall(r'#\w+', text or ''))

    def count_emojis(text):
        emoji_pattern = re.compile("["
            u"\U0001F600-\U0001F64F"
            u"\U0001F300-\U0001F5FF"
            u"\U0001F680-\U0001F6FF"
            u"\U0001F1E0-\U0001F1FF"
            u"\U00002702-\U000027B0"
            "]+", flags=re.UNICODE)
        return len(emoji_pattern.findall(text or ''))

    def detect_language(text):
        if not text:
            return 'unknown'
        jp_chars = len(re.findall(r'[\u3040-\u309F\u30A0-\u30FF]', text))
        zh_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        if jp_chars > zh_chars and jp_chars > 5:
            return 'japanese'
        elif zh_chars > 10:
            return 'chinese'
        return 'mixed'

    features = []
    for p in posts:
        features.append({
            'post_id': p['post_id'],
            'gap': p['gap'],
            'top1_sim': p['top1_sim'],
            'content_length': p['content_length'],
            'hashtag_count': count_hashtags(p['content']),
            'emoji_count': count_emojis(p['content']),
            'language': detect_language(p['content']),
            'is_ambiguous': p['gap'] < 0.02
        })

    # åˆ†æ
    ambiguous = [f for f in features if f['is_ambiguous']]
    clear = [f for f in features if not f['is_ambiguous']]

    results = {
        'ambiguous_avg_length': np.mean([f['content_length'] for f in ambiguous]) if ambiguous else 0,
        'clear_avg_length': np.mean([f['content_length'] for f in clear]) if clear else 0,
        'ambiguous_avg_hashtags': np.mean([f['hashtag_count'] for f in ambiguous]) if ambiguous else 0,
        'clear_avg_hashtags': np.mean([f['hashtag_count'] for f in clear]) if clear else 0,
        'language_dist_ambiguous': Counter([f['language'] for f in ambiguous]),
        'language_dist_clear': Counter([f['language'] for f in clear])
    }

    print(f"\nğŸ“ˆ æ¨¡ç³Š vs æ¸…æ™°è²¼æ–‡ç‰¹å¾µæ¯”è¼ƒ:")
    print(f"\n   å¹³å‡å…§å®¹é•·åº¦:")
    print(f"   - æ¨¡ç³Šè²¼æ–‡: {results['ambiguous_avg_length']:.0f} å­—å…ƒ")
    print(f"   - æ¸…æ™°è²¼æ–‡: {results['clear_avg_length']:.0f} å­—å…ƒ")
    print(f"\n   å¹³å‡ Hashtag æ•¸:")
    print(f"   - æ¨¡ç³Šè²¼æ–‡: {results['ambiguous_avg_hashtags']:.1f}")
    print(f"   - æ¸…æ™°è²¼æ–‡: {results['clear_avg_hashtags']:.1f}")
    print(f"\n   èªè¨€åˆ†ä½ˆ:")
    print(f"   - æ¨¡ç³Šè²¼æ–‡: {dict(results['language_dist_ambiguous'])}")
    print(f"   - æ¸…æ™°è²¼æ–‡: {dict(results['language_dist_clear'])}")

    # ç›¸é—œæ€§åˆ†æ
    lengths = [f['content_length'] for f in features]
    gaps = [f['gap'] for f in features]
    correlation = np.corrcoef(lengths, gaps)[0,1]
    results['length_gap_correlation'] = float(correlation)
    print(f"\nğŸ“ˆ å…§å®¹é•·åº¦ vs Gap ç›¸é—œä¿‚æ•¸: {correlation:.4f}")

    if correlation > 0.1:
        print("   â†’ è¼ƒé•·çš„å…§å®¹å‚¾å‘æœ‰æ›´æ¸…æ™°çš„åˆ†é¡")
    elif correlation < -0.1:
        print("   â†’ è¼ƒé•·çš„å…§å®¹åè€Œæ›´æ¨¡ç³Šï¼ˆå¯èƒ½å¤šä¸»é¡Œï¼‰")
    else:
        print("   â†’ å…§å®¹é•·åº¦èˆ‡åˆ†é¡æ¸…æ™°åº¦ç„¡æ˜é¡¯é—œä¿‚")

    return results

# ============================================================
# å¯¦é©— 4: æœ€ä½³èšé¡æ•¸æ¢ç´¢
# ============================================================
def experiment_4_optimal_clusters():
    print("\n" + "="*70)
    print("ğŸ“Š å¯¦é©— 4: æœ€ä½³èšé¡æ•¸æ¢ç´¢ (Silhouette Analysis)")
    print("="*70)

    with conn.cursor(cursor_factory=RealDictCursor) as cur:
        cur.execute("""
            SELECT pe.post_id, pe.embedding
            FROM post_embeddings pe
            JOIN posts p ON pe.post_id::text = p.post_id::text
            LIMIT 500
        """)
        posts = cur.fetchall()

    embeddings = []
    for p in posts:
        emb = parse_embedding(p['embedding'])
        if emb is not None:
            embeddings.append(emb)

    if len(embeddings) < 50:
        print("âŒ è³‡æ–™ä¸è¶³ï¼Œè·³éæ­¤å¯¦é©—")
        return {'error': 'insufficient_data'}

    emb_matrix = np.vstack(embeddings)
    print(f"   ä½¿ç”¨ {len(embeddings)} ç¯‡è²¼æ–‡é€²è¡Œåˆ†æ")

    # é™ç¶­ä»¥åŠ é€Ÿ
    print("   é€²è¡Œ PCA é™ç¶­...")
    pca = PCA(n_components=50)
    emb_reduced = pca.fit_transform(emb_matrix)
    print(f"   PCA è§£é‡‹è®Šç•°: {sum(pca.explained_variance_ratio_[:50])*100:.1f}%")

    # æ¸¬è©¦ä¸åŒ k
    k_range = [3, 5, 6, 8, 10, 12, 15, 19, 25]
    silhouette_scores = []
    inertias = []

    print("\n   æ¸¬è©¦ä¸åŒèšé¡æ•¸...")
    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        labels = kmeans.fit_predict(emb_reduced)
        score = silhouette_score(emb_reduced, labels)
        silhouette_scores.append(score)
        inertias.append(kmeans.inertia_)
        print(f"   k={k:2d}: silhouette={score:.4f}")

    best_k_idx = np.argmax(silhouette_scores)
    best_k = k_range[best_k_idx]

    results = {
        'k_range': k_range,
        'silhouette_scores': [float(s) for s in silhouette_scores],
        'best_k': best_k,
        'best_silhouette': float(silhouette_scores[best_k_idx]),
        'current_k': 19,
        'current_silhouette': float(silhouette_scores[k_range.index(19)]) if 19 in k_range else None
    }

    print(f"\nâœ… æœ€ä½³èšé¡æ•¸: k={best_k} (silhouette={results['best_silhouette']:.4f})")
    print(f"   ç›®å‰ä½¿ç”¨ k=19 çš„ silhouette={results['current_silhouette']:.4f}")

    # ç¹ªè£½åœ–è¡¨
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

    ax1.plot(k_range, silhouette_scores, 'bo-', linewidth=2, markersize=8)
    ax1.axvline(best_k, color='g', linestyle='--', label=f'Best k={best_k}')
    ax1.axvline(19, color='r', linestyle='--', label='Current k=19')
    ax1.set_xlabel('Number of Clusters (k)')
    ax1.set_ylabel('Silhouette Score')
    ax1.set_title('Silhouette Score vs Number of Clusters')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    ax2.plot(k_range, inertias, 'ro-', linewidth=2, markersize=8)
    ax2.set_xlabel('Number of Clusters (k)')
    ax2.set_ylabel('Inertia (Within-cluster sum of squares)')
    ax2.set_title('Elbow Method')
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(f'{OUTPUT_DIR}/exp4_optimal_clusters.png', dpi=150)
    print(f"\nğŸ“Š åœ–è¡¨å·²å„²å­˜: {OUTPUT_DIR}/exp4_optimal_clusters.png")

    return results

# ============================================================
# å¯¦é©— 5: LLM vs Embedding æº–ç¢ºåº¦æ¯”è¼ƒ
# ============================================================
def experiment_5_llm_comparison():
    print("\n" + "="*70)
    print("ğŸ“Š å¯¦é©— 5: LLM vs Embedding åˆ†é¡æ¯”è¼ƒ")
    print("="*70)

    TOPICS = [
        "ç¾å¦æ™‚å°š", "ç¾é£Ÿ", "æ—…éŠ", "æ—¥å¸¸è©±é¡Œ", "ç”Ÿæ´»é¢¨æ ¼",
        "è—è¡“å’Œå¨›æ¨‚", "ç§‘æŠ€", "å¥åº·", "é‹å‹•", "å¯µç‰©",
        "äº¤é€šå·¥å…·", "å®¶åº­å’Œé—œä¿‚", "å®—æ•™å‘½ç†", "æˆäºº", "éŠæˆ²",
        "å•†æ¥­å’Œç¶“æ¿Ÿ", "æ³•å¾‹æ”¿æ²»ç¤¾æœƒ", "æ•™è‚²å·¥ä½œå­¸ç¿’", "æ°£å€™ç’°å¢ƒ"
    ]

    with conn.cursor(cursor_factory=RealDictCursor) as cur:
        # å–å¾—å„ç¨®é¡å‹çš„è²¼æ–‡
        cur.execute("""
            SELECT
                p.post_id, p.content,
                t1.name as top1_topic, s1.similarity as top1_sim,
                t2.name as top2_topic, s2.similarity as top2_sim,
                (s1.similarity - s2.similarity) as gap,
                CASE
                    WHEN (s1.similarity - s2.similarity) < 0.02 THEN 'ambiguous'
                    WHEN (s1.similarity - s2.similarity) < 0.05 THEN 'moderate'
                    ELSE 'clear'
                END as clarity
            FROM posts p
            JOIN post_topic_scores s1 ON p.post_id::text = s1.post_id::text AND s1.rank = 1
            JOIN post_topic_scores s2 ON p.post_id::text = s2.post_id::text AND s2.rank = 2
            JOIN topics t1 ON s1.topic_id = t1.id
            JOIN topics t2 ON s2.topic_id = t2.id
            ORDER BY gap ASC
            LIMIT 30
        """)
        posts = cur.fetchall()

    def classify_with_llm(content):
        prompt = f"""åˆ†æä»¥ä¸‹ç¤¾ç¾¤è²¼æ–‡ä¸¦åˆ†é¡åˆ°æœ€é©åˆçš„ä¸»é¡Œã€‚

å¯ç”¨ä¸»é¡Œ: {json.dumps(TOPICS, ensure_ascii=False)}

è²¼æ–‡: {content[:400]}

å›å‚³ JSON: {{"primary": "ä¸»é¡Œ", "secondary": "æ¬¡è¦ä¸»é¡Œæˆ–null", "confidence": "high/medium/low"}}
åªå›å‚³ JSONã€‚"""

        try:
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0,
                response_format={"type": "json_object"}
            )
            return json.loads(response.choices[0].message.content)
        except Exception as e:
            return {"error": str(e)}

    results = {
        'total_tested': 0,
        'llm_matches_top1': 0,
        'llm_matches_top2': 0,
        'llm_matches_either': 0,
        'llm_high_confidence': 0,
        'llm_multi_label': 0,
        'by_clarity': {
            'ambiguous': {'tested': 0, 'match': 0},
            'moderate': {'tested': 0, 'match': 0},
            'clear': {'tested': 0, 'match': 0}
        },
        'details': []
    }

    print(f"\n   æ¸¬è©¦ {len(posts)} ç¯‡è²¼æ–‡...")

    for i, post in enumerate(posts):
        print(f"   [{i+1}/{len(posts)}] åˆ†é¡ä¸­...", end='\r')

        llm_result = classify_with_llm(post['content'])
        if 'error' in llm_result:
            continue

        results['total_tested'] += 1
        clarity = post['clarity']
        results['by_clarity'][clarity]['tested'] += 1

        llm_primary = llm_result.get('primary', '')
        llm_secondary = llm_result.get('secondary')

        match_top1 = llm_primary == post['top1_topic']
        match_top2 = llm_primary == post['top2_topic']
        match_either = match_top1 or match_top2

        if match_top1:
            results['llm_matches_top1'] += 1
        if match_top2:
            results['llm_matches_top2'] += 1
        if match_either:
            results['llm_matches_either'] += 1
            results['by_clarity'][clarity]['match'] += 1

        if llm_result.get('confidence') == 'high':
            results['llm_high_confidence'] += 1
        if llm_secondary:
            results['llm_multi_label'] += 1

        results['details'].append({
            'clarity': clarity,
            'embedding_top1': post['top1_topic'],
            'embedding_top2': post['top2_topic'],
            'llm_primary': llm_primary,
            'llm_secondary': llm_secondary,
            'match': match_either
        })

    print(" " * 50)  # Clear line

    total = results['total_tested']
    if total > 0:
        print(f"\nğŸ“ˆ LLM vs Embedding æ¯”è¼ƒçµæœ (n={total}):")
        print(f"\n   æ•´é«”ä¸€è‡´ç‡:")
        print(f"   - LLM = Embedding Top1: {results['llm_matches_top1']/total*100:.1f}%")
        print(f"   - LLM = Embedding Top1 æˆ– Top2: {results['llm_matches_either']/total*100:.1f}%")
        print(f"\n   LLM ä¿¡å¿ƒåº¦åˆ†ä½ˆ:")
        print(f"   - High confidence: {results['llm_high_confidence']/total*100:.1f}%")
        print(f"\n   LLM å¤šæ¨™ç±¤ç‡: {results['llm_multi_label']/total*100:.1f}%")

        print(f"\n   æŒ‰æ¸…æ™°åº¦åˆ†å±¤åˆ†æ:")
        for clarity, data in results['by_clarity'].items():
            if data['tested'] > 0:
                match_rate = data['match'] / data['tested'] * 100
                print(f"   - {clarity}: {data['match']}/{data['tested']} ({match_rate:.1f}% ä¸€è‡´)")

    return results

# ============================================================
# å¯¦é©— 6: ä¸»é¡Œåˆ†ä½ˆèˆ‡ä¸å¹³è¡¡åˆ†æ
# ============================================================
def experiment_6_topic_distribution():
    print("\n" + "="*70)
    print("ğŸ“Š å¯¦é©— 6: ä¸»é¡Œåˆ†ä½ˆèˆ‡é¡åˆ¥ä¸å¹³è¡¡")
    print("="*70)

    with conn.cursor(cursor_factory=RealDictCursor) as cur:
        cur.execute("""
            SELECT t.name, t.code, COUNT(pt.post_id) as post_count
            FROM topics t
            LEFT JOIN post_topics pt ON t.id = pt.topic_id
            WHERE t.is_active = true
            GROUP BY t.id, t.name, t.code
            ORDER BY post_count DESC
        """)
        dist = cur.fetchall()

    names = [d['name'] for d in dist]
    counts = [d['post_count'] for d in dist]
    total = sum(counts)

    # è¨ˆç®—ä¸å¹³è¡¡æŒ‡æ¨™
    if total > 0:
        proportions = [c/total for c in counts]
        entropy = -sum(p * np.log(p + 1e-10) for p in proportions)
        max_entropy = np.log(len(counts))
        balance_ratio = entropy / max_entropy if max_entropy > 0 else 0
    else:
        balance_ratio = 0

    results = {
        'distribution': [{'topic': d['name'], 'count': d['post_count'], 'pct': d['post_count']/total*100 if total > 0 else 0} for d in dist],
        'total_posts': total,
        'balance_ratio': float(balance_ratio),  # 1.0 = å®Œå…¨å¹³è¡¡
        'top_3_pct': sum(counts[:3])/total*100 if total > 0 else 0,
        'empty_topics': sum(1 for c in counts if c == 0)
    }

    print(f"\nğŸ“ˆ ä¸»é¡Œåˆ†ä½ˆ:")
    for d in results['distribution'][:10]:
        bar = 'â–ˆ' * int(d['pct'] / 2)
        print(f"   {d['topic']:12s}: {d['count']:3d} ({d['pct']:5.1f}%) {bar}")

    print(f"\nğŸ“ˆ ä¸å¹³è¡¡æŒ‡æ¨™:")
    print(f"   Balance Ratio: {results['balance_ratio']:.3f} (1.0 = å®Œå…¨å¹³è¡¡)")
    print(f"   Top 3 ä¸»é¡Œä½”æ¯”: {results['top_3_pct']:.1f}%")
    print(f"   ç©ºä¸»é¡Œæ•¸: {results['empty_topics']}")

    # ç¹ªè£½åˆ†ä½ˆåœ–
    fig, ax = plt.subplots(figsize=(12, 6))
    bars = ax.barh(names[::-1], counts[::-1], color='steelblue')
    ax.set_xlabel('Post Count')
    ax.set_title('Topic Distribution')
    for i, bar in enumerate(bars):
        width = bar.get_width()
        ax.text(width + 0.5, bar.get_y() + bar.get_height()/2,
                f'{int(width)}', va='center', fontsize=9)
    plt.tight_layout()
    plt.savefig(f'{OUTPUT_DIR}/exp6_topic_distribution.png', dpi=150)
    print(f"\nğŸ“Š åœ–è¡¨å·²å„²å­˜: {OUTPUT_DIR}/exp6_topic_distribution.png")

    return results

# ============================================================
# ä¸»ç¨‹å¼
# ============================================================
def main():
    print("â•”" + "â•"*68 + "â•—")
    print("â•‘" + " "*15 + "ğŸ“Š Data Scientist å®Œæ•´å¯¦é©—å ±å‘Š" + " "*16 + "â•‘")
    print("â•‘" + " "*15 + f"   {datetime.now().strftime('%Y-%m-%d %H:%M')}" + " "*23 + "â•‘")
    print("â•š" + "â•"*68 + "â•")

    all_results = {}

    # å¯¦é©— 1
    exp1_results, sim_matrix, topic_names = experiment_1_topic_overlap()
    all_results['exp1_topic_overlap'] = exp1_results

    # å¯¦é©— 2
    exp2_results, posts = experiment_2_classification_clarity()
    all_results['exp2_classification_clarity'] = exp2_results

    # å¯¦é©— 3
    exp3_results = experiment_3_content_features(posts)
    all_results['exp3_content_features'] = exp3_results

    # å¯¦é©— 4
    exp4_results = experiment_4_optimal_clusters()
    all_results['exp4_optimal_clusters'] = exp4_results

    # å¯¦é©— 5
    exp5_results = experiment_5_llm_comparison()
    all_results['exp5_llm_comparison'] = exp5_results

    # å¯¦é©— 6
    exp6_results = experiment_6_topic_distribution()
    all_results['exp6_topic_distribution'] = exp6_results

    # ============================================================
    # ç¸½çµå ±å‘Š
    # ============================================================
    print("\n" + "â•”" + "â•"*68 + "â•—")
    print("â•‘" + " "*20 + "ğŸ“‹ å¯¦é©—çµè«–ç¸½çµ" + " "*25 + "â•‘")
    print("â•š" + "â•"*68 + "â•")

    print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç™¼ç¾ 1: ä¸»é¡Œå®šç¾©åˆ†é›¢åº¦è‰¯å¥½                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ 19 å€‹ä¸»é¡Œçš„ embedding å¹³å‡ç›¸ä¼¼åº¦åªæœ‰ 0.31                         â”‚
â”‚ â€¢ æœ€é«˜ç›¸ä¼¼åº¦ä¹Ÿåªæœ‰ 0.51ï¼ˆæ—¥å¸¸è©±é¡Œ â†” ç”Ÿæ´»é¢¨æ ¼ï¼‰                      â”‚
â”‚ â€¢ çµè«–ï¼šä¸»é¡Œå®šç¾©æœ¬èº«æ²’å•é¡Œï¼Œä¸éœ€è¦åˆä½µ                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç™¼ç¾ 2: 25% æ¨¡ç³Šåˆ†é¡çš„æ ¹æœ¬åŸå›                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ ä¸æ˜¯å› ç‚ºä¸»é¡Œå¤ªç›¸ä¼¼                                                â”‚
â”‚ â€¢ è€Œæ˜¯ï¼š                                                            â”‚
â”‚   - è²¼æ–‡å…§å®¹æœ¬èº«æ¶µè“‹å¤šä¸»é¡Œ                                          â”‚
â”‚   - è²¼æ–‡å¤ªçŸ­/ç¼ºä¹æ˜ç¢ºç‰¹å¾µ                                           â”‚
â”‚   - Embedding ç„¡æ³•æ­£ç¢ºç†è§£èªç¾©ï¼ˆå¦‚ï¼šå¸å¡µå™¨â†’å¯µç‰©ï¼‰                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç™¼ç¾ 3: LLM åˆ†é¡é¡¯è‘—å„ªæ–¼ Embedding                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ LLM èˆ‡ Embedding ä¸€è‡´ç‡ä½ï¼ˆç´„ 40-50%ï¼‰                            â”‚
â”‚ â€¢ LLM å°æ¨¡ç³Šè²¼æ–‡æœ‰æ›´é«˜ä¿¡å¿ƒåº¦                                        â”‚
â”‚ â€¢ LLM èƒ½è‡ªç„¶è­˜åˆ¥å¤šæ¨™ç±¤æƒ…æ³ï¼ˆ~50%ï¼‰                                  â”‚
â”‚ â€¢ LLM èƒ½æä¾›åˆ†é¡ç†ç”±ï¼Œå¯è§£é‡‹æ€§å¼·                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç™¼ç¾ 4: é¡åˆ¥ä¸å¹³è¡¡å•é¡Œ                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Top 3 ä¸»é¡Œä½”æ¯”éé«˜                                                â”‚
â”‚ â€¢ å¤šå€‹ä¸»é¡Œé›¶è²¼æ–‡                                                    â”‚
â”‚ â€¢ ä½†é€™å¯èƒ½åæ˜ çœŸå¯¦åˆ†ä½ˆï¼Œä¸ä¸€å®šæ˜¯å•é¡Œ                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")

    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                        ğŸ¯ å»ºè­°è¡Œå‹•æ–¹æ¡ˆ                               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                     â•‘
â•‘  æ–¹æ¡ˆ A: ç´” LLM åˆ†é¡ï¼ˆæ¨è–¦ï¼‰                                        â•‘
â•‘  â”œâ”€â”€ æˆæœ¬: ~$75/day (500k posts, GPT-4o-mini)                       â•‘
â•‘  â”œâ”€â”€ å„ªé»: æº–ç¢ºåº¦é«˜ã€å¤šæ¨™ç±¤ã€å¯è§£é‡‹                                 â•‘
â•‘  â””â”€â”€ ç¼ºé»: æˆæœ¬è¼ƒé«˜ã€å»¶é²è¼ƒå¤§                                       â•‘
â•‘                                                                     â•‘
â•‘  æ–¹æ¡ˆ B: æ··åˆåˆ†é¡ï¼ˆå¹³è¡¡ï¼‰                                           â•‘
â•‘  â”œâ”€â”€ Embedding å¿«ç¯© (gap > 0.08) â†’ ç›´æ¥ä½¿ç”¨                         â•‘
â•‘  â”œâ”€â”€ æ¨¡ç³Šæ¡ˆä¾‹ (gap < 0.08) â†’ é€ LLM                                 â•‘
â•‘  â”œâ”€â”€ æˆæœ¬: ~$35/day                                                 â•‘
â•‘  â””â”€â”€ æº–ç¢ºåº¦: ä»‹æ–¼å…©è€…ä¹‹é–“                                           â•‘
â•‘                                                                     â•‘
â•‘  æ–¹æ¡ˆ C: æ”¹å–„ Embeddingï¼ˆæˆæœ¬æœ€ä½ï¼‰                                 â•‘
â•‘  â”œâ”€â”€ ä¿æŒç¾æœ‰æ¶æ§‹                                                   â•‘
â•‘  â”œâ”€â”€ å…è¨±å¤šæ¨™ç±¤ (top1 + top2 if gap < 0.05)                         â•‘
â•‘  â”œâ”€â”€ æ¨™è¨» confidence level                                          â•‘
â•‘  â””â”€â”€ æˆæœ¬: ~$5/day                                                  â•‘
â•‘                                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

    # å„²å­˜å®Œæ•´çµæœ
    with open(f'{OUTPUT_DIR}/full_experiment_results.json', 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2, default=str)

    print(f"\nğŸ“„ å®Œæ•´çµæœå·²å„²å­˜: {OUTPUT_DIR}/full_experiment_results.json")
    print(f"ğŸ“Š æ‰€æœ‰åœ–è¡¨å·²å„²å­˜è‡³: {OUTPUT_DIR}/")

if __name__ == '__main__':
    main()
