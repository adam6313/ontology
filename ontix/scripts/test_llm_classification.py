#!/usr/bin/env python3
"""
æ¸¬è©¦ LLM åˆ†é¡ vs Embedding åˆ†é¡
"""

import psycopg2
from psycopg2.extras import RealDictCursor
import json
from openai import OpenAI

# è³‡æ–™åº«é€£ç·š
conn = psycopg2.connect(
    host="localhost",
    port=5432,
    dbname="ontix_dev",
    user="ontix",
    password="ontix_dev"
)

# OpenAI client
import yaml
with open('/Users/adam/poc/ontology/ontix/config/dev.yaml') as f:
    config = yaml.safe_load(f)
client = OpenAI(api_key=config['openai_api_key'])

TOPICS = [
    "ç¾å¦æ™‚å°š", "ç¾é£Ÿ", "æ—…éŠ", "æ—¥å¸¸è©±é¡Œ", "ç”Ÿæ´»é¢¨æ ¼",
    "è—è¡“å’Œå¨›æ¨‚", "ç§‘æŠ€", "å¥åº·", "é‹å‹•", "å¯µç‰©",
    "äº¤é€šå·¥å…·", "å®¶åº­å’Œé—œä¿‚", "å®—æ•™å‘½ç†", "æˆäºº", "éŠæˆ²",
    "å•†æ¥­å’Œç¶“æ¿Ÿ", "æ³•å¾‹æ”¿æ²»ç¤¾æœƒ", "æ•™è‚²å·¥ä½œå­¸ç¿’", "æ°£å€™ç’°å¢ƒ"
]

def get_ambiguous_posts(limit=10):
    """å–å¾—æ¨¡ç³Šåˆ†é¡çš„è²¼æ–‡"""
    with conn.cursor(cursor_factory=RealDictCursor) as cur:
        cur.execute("""
            SELECT
                p.post_id,
                p.content,
                t1.name as top1_topic,
                s1.similarity as top1_sim,
                t2.name as top2_topic,
                s2.similarity as top2_sim,
                (s1.similarity - s2.similarity) as gap
            FROM posts p
            JOIN post_topic_scores s1 ON p.post_id::text = s1.post_id::text AND s1.rank = 1
            JOIN post_topic_scores s2 ON p.post_id::text = s2.post_id::text AND s2.rank = 2
            JOIN topics t1 ON s1.topic_id = t1.id
            JOIN topics t2 ON s2.topic_id = t2.id
            WHERE (s1.similarity - s2.similarity) < 0.02
            ORDER BY gap ASC
            LIMIT %s
        """, (limit,))
        return cur.fetchall()

def classify_with_llm(content: str) -> dict:
    """ä½¿ç”¨ LLM åˆ†é¡"""
    prompt = f"""ä½ æ˜¯ç¤¾ç¾¤è²¼æ–‡åˆ†é¡å°ˆå®¶ã€‚è«‹åˆ†æä»¥ä¸‹è²¼æ–‡ä¸¦åˆ†é¡ã€‚

å¯ç”¨çš„ä¸»é¡Œé¡åˆ¥ï¼š
{json.dumps(TOPICS, ensure_ascii=False)}

è²¼æ–‡å…§å®¹ï¼š
{content[:500]}

è«‹å›å‚³ JSON æ ¼å¼ï¼š
{{
    "primary_topic": "ä¸»è¦ä¸»é¡Œ",
    "secondary_topic": "æ¬¡è¦ä¸»é¡Œï¼ˆå¦‚æœæœ‰çš„è©±ï¼Œæ²’æœ‰å°±å¡« nullï¼‰",
    "confidence": "high/medium/low",
    "reason": "ç°¡çŸ­èªªæ˜åˆ†é¡ç†ç”±"
}}

åªå›å‚³ JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        response_format={"type": "json_object"}
    )

    return json.loads(response.choices[0].message.content)

def main():
    print("ğŸ“Š å–å¾—æ¨¡ç³Šåˆ†é¡è²¼æ–‡...")
    posts = get_ambiguous_posts(10)
    print(f"   æ‰¾åˆ° {len(posts)} ç¯‡æ¨¡ç³Šè²¼æ–‡\n")

    if not posts:
        print("âŒ æ²’æœ‰æ‰¾åˆ°æ¨¡ç³Šåˆ†é¡çš„è²¼æ–‡")
        return

    print("=" * 80)
    print("ğŸ”¬ Embedding vs LLM åˆ†é¡æ¯”è¼ƒ")
    print("=" * 80)

    results = []
    for i, post in enumerate(posts, 1):
        print(f"\n--- è²¼æ–‡ {i} (gap: {post['gap']:.4f}) ---")
        print(f"å…§å®¹: {post['content'][:100]}...")
        print(f"\nğŸ“Š Embedding åˆ†é¡:")
        print(f"   Top1: {post['top1_topic']} ({post['top1_sim']:.3f})")
        print(f"   Top2: {post['top2_topic']} ({post['top2_sim']:.3f})")

        print(f"\nğŸ¤– LLM åˆ†é¡:")
        try:
            llm_result = classify_with_llm(post['content'])
            print(f"   ä¸»é¡Œ: {llm_result['primary_topic']}")
            if llm_result.get('secondary_topic'):
                print(f"   æ¬¡è¦: {llm_result['secondary_topic']}")
            print(f"   ä¿¡å¿ƒ: {llm_result['confidence']}")
            print(f"   ç†ç”±: {llm_result['reason']}")

            results.append({
                'post_id': post['post_id'],
                'embedding_top1': post['top1_topic'],
                'embedding_top2': post['top2_topic'],
                'llm_primary': llm_result['primary_topic'],
                'llm_secondary': llm_result.get('secondary_topic'),
                'llm_confidence': llm_result['confidence'],
                'match': llm_result['primary_topic'] == post['top1_topic'] or
                         llm_result['primary_topic'] == post['top2_topic']
            })
        except Exception as e:
            print(f"   âŒ éŒ¯èª¤: {e}")

    # çµ±è¨ˆ
    print("\n" + "=" * 80)
    print("ğŸ“ˆ çµ±è¨ˆçµæœ")
    print("=" * 80)

    if results:
        match_count = sum(1 for r in results if r['match'])
        print(f"\nLLM çµæœèˆ‡ Embedding Top1/Top2 ä¸€è‡´: {match_count}/{len(results)} ({match_count/len(results)*100:.0f}%)")

        high_conf = sum(1 for r in results if r['llm_confidence'] == 'high')
        med_conf = sum(1 for r in results if r['llm_confidence'] == 'medium')
        low_conf = sum(1 for r in results if r['llm_confidence'] == 'low')
        print(f"LLM ä¿¡å¿ƒåº¦åˆ†ä½ˆ: high={high_conf}, medium={med_conf}, low={low_conf}")

        multi_label = sum(1 for r in results if r['llm_secondary'])
        print(f"LLM èªç‚ºæ˜¯å¤šæ¨™ç±¤: {multi_label}/{len(results)} ({multi_label/len(results)*100:.0f}%)")

if __name__ == '__main__':
    main()
