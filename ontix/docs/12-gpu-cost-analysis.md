# GPU 自建 vs GPT API 成本分析報告

> 撰寫日期：2026-02-09 | Ontix 專案工作負載評估

---

## 1. 摘要（Executive Summary）

| 項目 | GPT API (gpt-4o-mini) | GPT API (gpt-4o) | GPU 自建 (120B Q4) |
|------|----------------------|-------------------|-------------------|
| **日成本（10 萬篇）** | ~$204 | ~$3,394 | ~$58（電費） |
| **月成本** | ~$6,130 | ~$101,838 | ~$1,750（電費） |
| **年成本** | ~$73,560 | ~$1,222,056 | ~$21,000（電費） |
| **首年 TCO** | $73,560 | $1,222,056 | $220,000～$370,000 |
| **兩年 TCO** | $147,120 | $2,444,112 | $241,000～$391,000 |
| **三年 TCO** | $220,680 | $3,666,168 | $262,000～$412,000 |

### 關鍵結論

1. **gpt-4o-mini 是目前最具成本效益的方案**——年費 $73K，零維運負擔，品質足夠
2. **GPU 自建在 1.5～3 年回本**——取決於硬體規格（A6000 方案 ~1.5 年，H100 方案 ~3 年）
3. **gpt-4o 極度昂貴**——年費 $1.2M，除非必須使用大模型品質，否則不建議
4. **建議：短期用 gpt-4o-mini，中期規劃 GPU 自建 + API 備援**

---

## 2. 工作負載分析

### 2.1 每篇貼文 LLM 呼叫

從程式碼精確測量（`internal/infra/openai/openai.go`）：

| 呼叫 | 模型 | 輸入 Token（估算） | 輸出 MaxTokens | 溫度 |
|------|------|-------------------|----------------|------|
| 實體抽取 | gpt-4o-mini | ~600 | 1,500 | 0.1 |
| 貼文分析 | gpt-4o-mini | ~400 | 800 | 0.1 |
| 標籤生成 | gpt-4o-mini | ~300 | 500 | 0.2 |
| 向量化 | text-embedding-3-small | ~300 | N/A | N/A |
| **每篇合計** | | **~1,600 input** | **~2,800 output** | |

> 📌 實際 output 通常遠低於 max_tokens（JSON 回應平均使用 30-50%），
> 以保守 60% 使用率估算：**~1,680 output tokens/篇**

### 2.2 日/月總量

| 指標 | 5 萬篇/天 | 10 萬篇/天 |
|------|----------|-----------|
| Input tokens/天 | 80M | 160M |
| Output tokens/天 | 84M | 168M |
| Embed tokens/天 | 15M | 30M |
| **月 Input** | 2.4B | 4.8B |
| **月 Output** | 2.52B | 5.04B |
| **月 Embed** | 450M | 900M |

---

## 3. 方案 A：OpenAI GPT API 成本

### 3.1 定價基準（2026 年 2 月）

| 模型 | Input（/1M tokens） | Output（/1M tokens） | Batch API |
|------|---------------------|----------------------|-----------|
| gpt-4o-mini | $0.150 | $0.600 | 50% 折扣 |
| gpt-4o | $2.500 | $10.000 | 50% 折扣 |
| text-embedding-3-small | $0.020 | N/A | $0.010 |

> 來源：[OpenAI Pricing](https://platform.openai.com/docs/pricing)

### 3.2 gpt-4o-mini（推薦的 API 方案）

**10 萬篇/天：**

| 項目 | 計算 | 日成本 |
|------|------|--------|
| Input | 160M × $0.15 / 1M | $24.00 |
| Output | 168M × $0.60 / 1M | $100.80 |
| Embedding | 30M × $0.02 / 1M | $0.60 |
| **日合計** | | **$125.40** |
| **月合計** | × 30 | **$3,762** |
| **年合計** | × 365 | **$45,771** |

**使用 Batch API（50% 折扣）：**

| 項目 | 計算 | 日成本 |
|------|------|--------|
| Input | 160M × $0.075 / 1M | $12.00 |
| Output | 168M × $0.30 / 1M | $50.40 |
| Embedding | 30M × $0.01 / 1M | $0.30 |
| **日合計** | | **$62.70** |
| **月合計** | × 30 | **$1,881** |
| **年合計** | × 365 | **$22,886** |

> ⚡ Batch API 適合我們的場景（Worker 批次處理，24 小時內完成即可）

**5 萬篇/天（減半）：**

| 方案 | 月成本 | 年成本 |
|------|--------|--------|
| 即時 API | $1,881 | $22,886 |
| Batch API | $941 | $11,443 |

### 3.3 gpt-4o（對標 120B 品質）

**10 萬篇/天：**

| 項目 | 計算 | 日成本 |
|------|------|--------|
| Input | 160M × $2.50 / 1M | $400.00 |
| Output | 168M × $10.00 / 1M | $1,680.00 |
| Embedding | 30M × $0.02 / 1M | $0.60 |
| **日合計** | | **$2,080.60** |
| **月合計** | × 30 | **$62,418** |
| **年合計** | × 365 | **$759,419** |

**使用 Batch API：**

| 項目 | 日成本 | 月成本 | 年成本 |
|------|--------|--------|--------|
| Batch API | $1,040.60 | $31,218 | $379,819 |

### 3.4 成本比較表

| 方案 | 日（10 萬篇） | 月 | 年 |
|------|--------------|------|------|
| gpt-4o-mini 即時 | $125 | $3,762 | $45,771 |
| gpt-4o-mini Batch | $63 | $1,881 | $22,886 |
| gpt-4o 即時 | $2,081 | $62,418 | $759,419 |
| gpt-4o Batch | $1,041 | $31,218 | $379,819 |

---

## 4. 方案 B：地端 GPU 自建 120B 開源模型

### 4.1 模型需求

| 項目 | 數值 |
|------|------|
| 模型參數量 | 120B（如 GPT-OSS-120B） |
| 量化方式 | Q4（4-bit） |
| 模型體積 | ~60-65 GB |
| 推論框架 | vLLM / SGLang |

### 4.2 GPU 方案比較

| GPU | VRAM | 頻寬 | 功耗 | 單卡價格 | 每 replica 所需卡數 |
|-----|------|------|------|---------|-------------------|
| RTX A6000 | 48 GB | 768 GB/s | 300W | $4,650 | 2 張（TP=2） |
| RTX 6000 Ada | 48 GB | 960 GB/s | 300W | $6,800 | 2 張（TP=2） |
| H100 PCIe | 80 GB | 2,039 GB/s | 350W | $25,000 | 1 張 |
| H100 SXM | 80 GB | 3,350 GB/s | 700W | $35,000 | 1 張 |

> 來源：[NVIDIA Marketplace](https://marketplace.nvidia.com)、[Jarvislabs H100 Guide](https://docs.jarvislabs.ai/blog/h100-price)

### 4.3 吞吐量估算

基於公開 benchmark 數據：

| 配置 | 吞吐量（tokens/s） | 來源 |
|------|-------------------|------|
| 2× H100（FP16, vLLM） | ~4,742（100 併發） | [Clarifai Benchmark](https://www.clarifai.com/blog/comparing-sglang-vllm-and-tensorrt-llm-with-gpt-oss-120b) |
| 2× H100（FP16, SGLang） | ~3,109（50 併發） | 同上 |
| 1× Pro 6000（Q4, vLLM） | ~1,779（50 併發） | [DatabaseMart Benchmark](https://www.databasemart.com/blog/vllm-gpu-benchmark-pro6000) |
| 4× A6000（70B FP16, vLLM） | ~450（50 併發） | [DatabaseMart 4xA6000](https://www.databasemart.com/blog/vllm-gpu-benchmark-a6000-4) |

**我們的需求：**

```
10 萬篇/天 × ~4,476 tokens/篇 = 4.476 億 tokens/天
= 4.476 億 / 86,400 秒 ≈ 5,181 tokens/秒（持續吞吐）
```

> 加上高峰餘裕（×1.5）→ 目標：**~7,700 tokens/秒**

### 4.4 各 GPU 方案所需數量

#### 方案 B1：RTX A6000（TP=2, Q4 量化）

| 項目 | 估算 |
|------|------|
| 每 replica 吞吐量 | ~400-600 tok/s（Q4, 2×A6000） |
| 所需 replica | 13-20 組 |
| 所需 GPU 數量 | **26-40 張** |
| GPU 成本 | $120,900 ~ $186,000 |
| 伺服器機箱（4U 8-GPU, ×4-5 台） | $22,000 ~ $30,000 |
| CPU + RAM + NVMe（每台） | ~$5,000 × 5 = $25,000 |
| 網路交換機 + 配件 | ~$5,000 |
| **硬體總計** | **$173,000 ~ $246,000** |

#### 方案 B2：RTX 6000 Ada（TP=2, Q4 量化）

| 項目 | 估算 |
|------|------|
| 每 replica 吞吐量 | ~600-900 tok/s（更高頻寬） |
| 所需 replica | 9-13 組 |
| 所需 GPU 數量 | **18-26 張** |
| GPU 成本 | $122,400 ~ $176,800 |
| 伺服器機箱（3-4 台） | $16,500 ~ $22,000 |
| CPU + RAM + NVMe | ~$5,000 × 4 = $20,000 |
| 網路 + 配件 | ~$5,000 |
| **硬體總計** | **$164,000 ~ $224,000** |

#### 方案 B3：H100 PCIe（單卡, FP16/Q4）

| 項目 | 估算 |
|------|------|
| 每卡吞吐量 | ~1,500-2,400 tok/s（Q4, 單卡） |
| 所需 GPU 數量 | **4-6 張** |
| GPU 成本 | $100,000 ~ $150,000 |
| 伺服器機箱（1-2 台） | $5,500 ~ $11,000 |
| CPU + RAM + NVMe | ~$5,000 × 2 = $10,000 |
| 網路 + 配件 | ~$3,000 |
| **硬體總計** | **$119,000 ~ $174,000** |

### 4.5 營運成本（電費 + 維護）

以台灣工業電費 TWD 3.5/kWh ≈ USD $0.11/kWh 估算：

| GPU 方案 | GPU 功耗 | 伺服器其他 | PUE 1.4 | 月電費 |
|---------|---------|-----------|---------|--------|
| B1: 40× A6000 | 12.0 kW | 3.0 kW | 21.0 kW | $1,663 |
| B2: 26× 6000 Ada | 7.8 kW | 2.4 kW | 14.3 kW | $1,132 |
| B3: 6× H100 PCIe | 2.1 kW | 1.0 kW | 4.3 kW | $344 |

> PUE (Power Usage Effectiveness) = 1.4（含冷卻、網路等設備）

| 方案 | 月電費 | 年電費 | 人力維運（年） | 年營運總成本 |
|------|--------|--------|-------------|------------|
| B1: A6000 | $1,663 | $19,956 | ~$5,000 | ~$25,000 |
| B2: 6000 Ada | $1,132 | $13,584 | ~$5,000 | ~$19,000 |
| B3: H100 | $344 | $4,128 | ~$5,000 | ~$9,000 |

---

## 5. TCO 比較（Total Cost of Ownership）

### 5.1 三年 TCO 明細（10 萬篇/天）

| | gpt-4o-mini Batch | gpt-4o-mini 即時 | B1: A6000 | B2: 6000 Ada | B3: H100 |
|------|-------------------|-----------------|-----------|-------------|----------|
| **硬體** | $0 | $0 | $210,000 | $194,000 | $147,000 |
| **第 1 年營運** | $22,886 | $45,771 | $25,000 | $19,000 | $9,000 |
| **第 2 年營運** | $22,886 | $45,771 | $25,000 | $19,000 | $9,000 |
| **第 3 年營運** | $22,886 | $45,771 | $25,000 | $19,000 | $9,000 |
| **3 年 TCO** | **$68,658** | **$137,313** | **$285,000** | **$251,000** | **$174,000** |

### 5.2 損益兩平分析

與 **gpt-4o-mini 即時 API（$45,771/年）** 比較：

| GPU 方案 | 硬體投資 | 年淨節省 | 回本時間 |
|---------|---------|---------|---------|
| B1: A6000 | $210,000 | $20,771 | **~10 年** ❌ |
| B2: 6000 Ada | $194,000 | $26,771 | **~7.2 年** ❌ |
| B3: H100 | $147,000 | $36,771 | **~4.0 年** ⚠️ |

與 **gpt-4o Batch API（$379,819/年）** 比較：

| GPU 方案 | 硬體投資 | 年淨節省 | 回本時間 |
|---------|---------|---------|---------|
| B1: A6000 | $210,000 | $354,819 | **~7 個月** ✅ |
| B2: 6000 Ada | $194,000 | $360,819 | **~6.5 個月** ✅ |
| B3: H100 | $147,000 | $370,819 | **~4.7 個月** ✅ |

### 5.3 重要觀察

> ⚠️ **gpt-4o-mini 的成本已經非常低**。如果 gpt-4o-mini 的品質足夠（目前 Ontix 使用中），
> GPU 自建在純成本上**很難回本**。
>
> 只有在以下情境下，GPU 自建才有明確的 ROI：
> 1. 需要 gpt-4o 級別的品質 → 開源 120B 對標 → 半年內回本
> 2. 資料主權/隱私合規 → 不允許數據出境
> 3. 延遲敏感 → 地端推論 < 50ms TTFT
> 4. 量級翻倍（20 萬篇+/天）→ API 成本線性增長，GPU 固定成本不變

---

## 6. 故障備援（Fallback）分析

### 6.1 地端 GPU 故障時的 API 燃燒率

假設地端 GPU 完全故障，需 100% fallback 到 OpenAI API：

| 備援方案 | 日燃燒率（10 萬篇） | 月燃燒率 | 說明 |
|---------|-------------------|---------|------|
| gpt-4o-mini 即時 | $125/天 | $3,762 | 最便宜，品質稍降 |
| gpt-4o-mini Batch | $63/天 | $1,881 | 最省，但有 24h 延遲 |
| gpt-4o 即時 | $2,081/天 | $62,418 | 與 120B 品質對標 |
| gpt-4o Batch | $1,041/天 | $31,218 | 品質對標 + 省一半 |

### 6.2 備援預算建議

| 情境 | 估算頻率 | 月備援預算 |
|------|---------|-----------|
| GPU 偶發重啟（<1h/月） | 0.1% 流量 fallback | ~$4 |
| 單台伺服器故障（修復 3 天） | 10% 流量 × 3 天 | ~$38 |
| 全面故障（修復 7 天） | 100% 流量 × 7 天 | ~$878 |
| 最壞情況（修復 30 天） | 100% 流量 × 30 天 | ~$1,881 |

> 💡 建議：每月預留 **$2,000 API 備援預算**（gpt-4o-mini Batch），可覆蓋最壞情況

---

## 7. 混合架構建議

### 7.1 推薦架構

```
┌─────────────────────────────────────────┐
│              Load Balancer              │
│         (nginx / HAProxy)               │
└────────────┬──────────────┬─────────────┘
             │              │
     ┌───────▼───────┐ ┌───▼──────────────┐
     │  地端 GPU 叢集  │ │  OpenAI API      │
     │  (主要推論)    │ │  (備援 fallback)  │
     │  vLLM serving  │ │  gpt-4o-mini     │
     │  120B Q4       │ │  Batch API       │
     └───────────────┘ └──────────────────┘
```

### 7.2 分階段實施路線

| 階段 | 時間 | 動作 | 月成本 |
|------|------|------|--------|
| **Phase 0**（現狀） | 立即 | 維持 gpt-4o-mini，評估 Batch API | ~$1,881 |
| **Phase 1** | 1-3 月 | 切換 Batch API，建立 token 監控 | ~$1,881 |
| **Phase 2** | 3-6 月 | 採購 GPU（建議 B3: H100 ×4-6） | $147K 一次性 |
| **Phase 3** | 6 月+ | 地端為主，API 備援 | ~$344（電）+ $2K（備援預算） |

### 7.3 最終建議

| 條件 | 建議方案 |
|------|---------|
| 品質 OK，預算有限 | **維持 gpt-4o-mini Batch API**（$23K/年） |
| 需對標 gpt-4o 品質 | **採購 H100 ×4-6** + API 備援（首年 $156K，之後 $11K/年） |
| 資料主權要求 | **採購 H100 ×4-6**（不可替代） |
| 量級 >20 萬篇/天 | **GPU 自建必要**（API 成本線性翻倍，GPU 可能免加購） |

---

## 附錄

### A. 計算假設

- Output token 實際使用率：max_tokens 的 60%
- GPU 使用率：80%（含模型載入、GC、排隊）
- PUE：1.4（中型機房）
- 台灣工業電費：TWD 3.5/kWh ≈ USD 0.11/kWh
- 人力維運：$5,000/年（兼職 DevOps，非專職）
- GPU 折舊：3 年直線折舊

### B. 定價來源

- OpenAI API：[platform.openai.com/docs/pricing](https://platform.openai.com/docs/pricing)（2026-02）
- GPU 價格：[NVIDIA Marketplace](https://marketplace.nvidia.com)、[Jarvislabs Guide](https://docs.jarvislabs.ai/blog/h100-price)
- GPU Benchmark：[Clarifai Blog](https://www.clarifai.com/blog/comparing-sglang-vllm-and-tensorrt-llm-with-gpt-oss-120b)、[DatabaseMart](https://www.databasemart.com/blog/vllm-gpu-benchmark-pro6000)
- 伺服器：[Supermicro eStore](https://store.supermicro.com/us_en/systems/gpu.html)

### C. 風險因素

| 風險 | 影響 | 緩解 |
|------|------|------|
| OpenAI 調價（降價趨勢） | API 方案更有利 | 每季重新評估 |
| GPU 缺貨/漲價 | 自建延遲 | 提早下單、考慮雲端 GPU |
| 開源模型品質不如 GPT | 品質下降 | 混合架構，關鍵任務用 API |
| 量級暴增 | 所有方案成本增加 | GPU 方案邊際成本更低 |
| 模型更新換代 | GPU 可能需要升級 | 3 年折舊週期對齊 |
