#!/usr/bin/env python3
"""
HDBSCAN Clustering åƒæ•¸å¯¦é©—
===========================
ç›®æ¨™ï¼šæ‰¾å‡ºæœ€ä½³çš„ min_cluster_size å’Œ min_samples åƒæ•¸

è©•ä¼°æŒ‡æ¨™ï¼š
1. Silhouette Score (è¼ªå»“ä¿‚æ•¸) - è¶Šé«˜è¶Šå¥½ï¼Œç¯„åœ [-1, 1]
2. DBCV Score (åŸºæ–¼å¯†åº¦çš„é©—è­‰) - HDBSCAN åŸç”ŸæŒ‡æ¨™
3. Noise Ratio (å™ªéŸ³æ¯”ä¾‹) - å¤ªé«˜è¡¨ç¤ºåƒæ•¸å¤ªåš´æ ¼
4. Cluster Count (ç¾¤é›†æ•¸é‡) - æ˜¯å¦åˆç†
5. Cluster Size Distribution - å„ç¾¤å¤§å°æ˜¯å¦å¹³è¡¡
"""

import sys
import json
import numpy as np
import psycopg2
from itertools import product
from dataclasses import dataclass
from typing import List, Tuple, Optional

# å˜—è©¦å°å…¥ HDBSCAN
try:
    import hdbscan
    HDBSCAN_AVAILABLE = True
except ImportError:
    HDBSCAN_AVAILABLE = False
    print("Warning: hdbscan not installed, using sklearn DBSCAN as fallback")

from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import normalize

@dataclass
class ExperimentResult:
    """å¯¦é©—çµæœ"""
    min_cluster_size: int
    min_samples: int
    n_clusters: int
    noise_count: int
    noise_ratio: float
    silhouette: Optional[float]
    cluster_sizes: List[int]

    def to_dict(self):
        return {
            'min_cluster_size': self.min_cluster_size,
            'min_samples': self.min_samples,
            'n_clusters': self.n_clusters,
            'noise_count': self.noise_count,
            'noise_ratio': round(self.noise_ratio, 3),
            'silhouette': round(self.silhouette, 3) if self.silhouette else None,
            'cluster_sizes': self.cluster_sizes,
            'avg_cluster_size': round(np.mean(self.cluster_sizes), 1) if self.cluster_sizes else 0,
            'size_std': round(np.std(self.cluster_sizes), 1) if self.cluster_sizes else 0,
        }


def get_embeddings_by_topic(topic_id: int = None) -> Tuple[List[str], np.ndarray]:
    """å¾è³‡æ–™åº«å–å¾— embeddings"""
    conn = psycopg2.connect(
        host="localhost",
        port=5432,
        database="ontix_dev",
        user="ontix",
        password="ontix_dev"
    )

    if topic_id:
        query = """
            SELECT p.post_id, pe.embedding
            FROM posts p
            JOIN post_embeddings pe ON p.post_id = pe.post_id
            JOIN post_topics pt ON p.post_id::bigint = pt.post_id
            WHERE pt.topic_id = %s
            AND pe.embedding IS NOT NULL
        """
        params = (topic_id,)
    else:
        query = """
            SELECT p.post_id, pe.embedding
            FROM posts p
            JOIN post_embeddings pe ON p.post_id = pe.post_id
            WHERE pe.embedding IS NOT NULL
        """
        params = None

    cur = conn.cursor()
    cur.execute(query, params)
    rows = cur.fetchall()
    conn.close()

    post_ids = [str(row[0]) for row in rows]

    # è™•ç† embedding æ ¼å¼ï¼ˆå¯èƒ½æ˜¯å­—ä¸²æˆ– listï¼‰
    embeddings_list = []
    for row in rows:
        emb = row[1]
        if isinstance(emb, str):
            # è§£æå­—ä¸²æ ¼å¼ "[0.1, 0.2, ...]"
            emb = json.loads(emb)
        embeddings_list.append(emb)

    embeddings = np.array(embeddings_list, dtype=np.float32)
    return post_ids, embeddings


def run_hdbscan_experiment(
    embeddings: np.ndarray,
    min_cluster_size: int,
    min_samples: int
) -> ExperimentResult:
    """åŸ·è¡Œå–®æ¬¡ HDBSCAN å¯¦é©—"""

    # L2 æ­£è¦åŒ–ï¼ˆç”¨æ–¼ cosine similarityï¼‰
    embeddings_normalized = normalize(embeddings)

    if HDBSCAN_AVAILABLE:
        clusterer = hdbscan.HDBSCAN(
            min_cluster_size=min_cluster_size,
            min_samples=min_samples,
            metric='euclidean',  # æ­£è¦åŒ–å¾Œ euclidean â‰ˆ cosine
            cluster_selection_method='eom'
        )
        labels = clusterer.fit_predict(embeddings_normalized)
    else:
        # Fallback to DBSCAN
        eps = 0.5  # å°æ‡‰ç´„ cosine similarity 0.75
        clusterer = DBSCAN(
            eps=eps,
            min_samples=min_samples,
            metric='euclidean'
        )
        labels = clusterer.fit_predict(embeddings_normalized)

    # è¨ˆç®—æŒ‡æ¨™
    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    noise_mask = labels == -1
    noise_count = np.sum(noise_mask)
    noise_ratio = noise_count / len(labels)

    # Silhouette scoreï¼ˆéœ€è¦è‡³å°‘ 2 å€‹ clusterï¼‰
    silhouette = None
    if n_clusters >= 2 and noise_count < len(labels):
        non_noise_mask = ~noise_mask
        if np.sum(non_noise_mask) > n_clusters:
            try:
                silhouette = silhouette_score(
                    embeddings_normalized[non_noise_mask],
                    labels[non_noise_mask]
                )
            except:
                pass

    # å„ cluster å¤§å°
    cluster_sizes = []
    for i in range(n_clusters):
        size = np.sum(labels == i)
        cluster_sizes.append(int(size))
    cluster_sizes.sort(reverse=True)

    return ExperimentResult(
        min_cluster_size=min_cluster_size,
        min_samples=min_samples,
        n_clusters=n_clusters,
        noise_count=int(noise_count),
        noise_ratio=noise_ratio,
        silhouette=silhouette,
        cluster_sizes=cluster_sizes
    )


def run_grid_search(
    embeddings: np.ndarray,
    min_cluster_sizes: List[int],
    min_samples_list: List[int]
) -> List[ExperimentResult]:
    """ç¶²æ ¼æœç´¢æœ€ä½³åƒæ•¸"""
    results = []

    for mcs, ms in product(min_cluster_sizes, min_samples_list):
        if ms > mcs:  # min_samples ä¸èƒ½å¤§æ–¼ min_cluster_size
            continue
        result = run_hdbscan_experiment(embeddings, mcs, ms)
        results.append(result)

    return results


def evaluate_and_rank(results: List[ExperimentResult], n_samples: int) -> List[dict]:
    """è©•ä¼°ä¸¦æ’åçµæœ"""
    scored_results = []

    for r in results:
        # è¨ˆç®—ç¶œåˆåˆ†æ•¸
        score = 0

        # 1. Silhouette score (æ¬Šé‡ 30%)
        if r.silhouette is not None and r.silhouette > 0:
            score += r.silhouette * 30

        # 2. å™ªéŸ³æ¯”ä¾‹æ‡²ç½° (ç›®æ¨™: 10-30%)
        if 0.1 <= r.noise_ratio <= 0.3:
            score += 25  # ç†æƒ³ç¯„åœ
        elif r.noise_ratio < 0.1:
            score += 15  # å¤ªå°‘å™ªéŸ³å¯èƒ½éæ“¬åˆ
        elif r.noise_ratio <= 0.5:
            score += 10  # å¯æ¥å—
        else:
            score += 0   # å™ªéŸ³å¤ªå¤š

        # 3. Cluster æ•¸é‡åˆç†æ€§ (ç›®æ¨™: 3-10 for 100 samples)
        expected_clusters = max(3, min(10, n_samples // 15))
        if r.n_clusters == 0:
            score += 0
        elif abs(r.n_clusters - expected_clusters) <= 2:
            score += 25
        elif abs(r.n_clusters - expected_clusters) <= 4:
            score += 15
        else:
            score += 5

        # 4. Cluster å¤§å°å¹³è¡¡åº¦ (æ¬Šé‡ 20%)
        if r.cluster_sizes and len(r.cluster_sizes) > 1:
            cv = np.std(r.cluster_sizes) / np.mean(r.cluster_sizes)  # è®Šç•°ä¿‚æ•¸
            if cv < 0.5:
                score += 20  # å¾ˆå¹³è¡¡
            elif cv < 1.0:
                score += 15
            else:
                score += 5
        elif r.n_clusters == 1:
            score += 10

        result_dict = r.to_dict()
        result_dict['score'] = round(score, 1)
        scored_results.append(result_dict)

    # æŒ‰åˆ†æ•¸æ’åº
    scored_results.sort(key=lambda x: x['score'], reverse=True)
    return scored_results


def print_results(results: List[dict], title: str):
    """æ‰“å°çµæœè¡¨æ ¼"""
    print(f"\n{'='*80}")
    print(f" {title}")
    print(f"{'='*80}")
    print(f"{'Rank':<5} {'MCS':<5} {'MS':<5} {'Clusters':<10} {'Noise':<10} {'Silhouette':<12} {'Score':<8} {'Sizes'}")
    print(f"{'-'*80}")

    for i, r in enumerate(results[:10], 1):
        sizes_str = str(r['cluster_sizes'][:5])
        if len(r['cluster_sizes']) > 5:
            sizes_str += '...'

        sil_str = f"{r['silhouette']:.3f}" if r['silhouette'] else "N/A"

        print(f"{i:<5} {r['min_cluster_size']:<5} {r['min_samples']:<5} "
              f"{r['n_clusters']:<10} {r['noise_ratio']:.1%}     "
              f"{sil_str:<12} {r['score']:<8} {sizes_str}")


def main():
    print("=" * 80)
    print(" HDBSCAN Clustering åƒæ•¸å¯¦é©—")
    print("=" * 80)

    # å–å¾—æ‰€æœ‰ embeddingsï¼ˆä¸åˆ† topicï¼‰
    print("\nğŸ“Š è¼‰å…¥è³‡æ–™...")
    post_ids, embeddings = get_embeddings_by_topic(topic_id=None)
    print(f"   ç¸½å…± {len(embeddings)} ç¯‡æ–‡ç« ")

    if len(embeddings) < 10:
        print("âŒ è³‡æ–™é‡ä¸è¶³ï¼Œè‡³å°‘éœ€è¦ 10 ç¯‡æ–‡ç« ")
        return

    # å®šç¾©åƒæ•¸æœç´¢ç©ºé–“
    n = len(embeddings)

    # æ ¹æ“šè³‡æ–™é‡å‹•æ…‹èª¿æ•´æœç´¢ç¯„åœ
    if n < 50:
        min_cluster_sizes = [3, 4, 5]
        min_samples_list = [2, 3]
    elif n < 100:
        min_cluster_sizes = [3, 4, 5, 6, 7]
        min_samples_list = [2, 3, 4, 5]
    elif n < 500:
        min_cluster_sizes = [5, 7, 10, 12, 15]
        min_samples_list = [3, 5, 7, 10]
    else:
        min_cluster_sizes = [10, 15, 20, 25, 30]
        min_samples_list = [5, 10, 15]

    print(f"\nğŸ”¬ åƒæ•¸æœç´¢ç©ºé–“:")
    print(f"   min_cluster_size: {min_cluster_sizes}")
    print(f"   min_samples: {min_samples_list}")

    # åŸ·è¡Œç¶²æ ¼æœç´¢
    print(f"\nâ³ åŸ·è¡Œå¯¦é©—...")
    results = run_grid_search(embeddings, min_cluster_sizes, min_samples_list)

    # è©•ä¼°ä¸¦æ’å
    ranked_results = evaluate_and_rank(results, n)

    # æ‰“å°çµæœ
    print_results(ranked_results, "å…¨åŸŸ Clustering çµæœ (æ‰€æœ‰æ–‡ç« )")

    # æœ€ä½³åƒæ•¸å»ºè­°
    if ranked_results:
        best = ranked_results[0]
        print(f"\n" + "=" * 80)
        print(f" ğŸ† æœ€ä½³åƒæ•¸å»ºè­°")
        print(f"=" * 80)
        print(f"""
    min_cluster_size = {best['min_cluster_size']}
    min_samples      = {best['min_samples']}

    é æœŸçµæœ:
    - Clusters: {best['n_clusters']} å€‹
    - Noise: {best['noise_ratio']:.1%}
    - Silhouette: {best['silhouette'] if best['silhouette'] else 'N/A'}
    - å¹³å‡ Cluster å¤§å°: {best['avg_cluster_size']}
""")

    # æŒ‰ Topic åˆ†åˆ¥å¯¦é©—
    print(f"\n" + "=" * 80)
    print(f" æŒ‰ Topic åˆ†åˆ¥å¯¦é©—")
    print(f"=" * 80)

    # å–å¾—æœ‰è¶³å¤ è³‡æ–™çš„ topics
    conn = psycopg2.connect(
        host="localhost", port=5432, database="ontix_dev",
        user="ontix", password="ontix_dev"
    )
    cur = conn.cursor()
    cur.execute("""
        SELECT pt.topic_id, t.name, COUNT(*) as cnt
        FROM post_topics pt
        JOIN topics t ON pt.topic_id = t.id
        GROUP BY pt.topic_id, t.name
        HAVING COUNT(*) >= 10
        ORDER BY cnt DESC
    """)
    topics = cur.fetchall()
    conn.close()

    topic_results = {}
    for topic_id, topic_name, count in topics:
        print(f"\nğŸ“ Topic {topic_id}: {topic_name} ({count} ç¯‡)")

        _, topic_embeddings = get_embeddings_by_topic(topic_id)

        if len(topic_embeddings) < 10:
            print("   âš ï¸ è³‡æ–™ä¸è¶³ï¼Œè·³é")
            continue

        # é‡å°å°è³‡æ–™é›†èª¿æ•´åƒæ•¸
        if count < 30:
            mcs_list = [3, 4, 5]
            ms_list = [2, 3]
        else:
            mcs_list = [3, 5, 7]
            ms_list = [2, 3, 5]

        topic_exp_results = run_grid_search(topic_embeddings, mcs_list, ms_list)
        ranked = evaluate_and_rank(topic_exp_results, len(topic_embeddings))

        if ranked:
            best = ranked[0]
            topic_results[topic_id] = {
                'name': topic_name,
                'count': count,
                'best_params': {
                    'min_cluster_size': best['min_cluster_size'],
                    'min_samples': best['min_samples']
                },
                'expected_clusters': best['n_clusters'],
                'noise_ratio': best['noise_ratio'],
                'silhouette': best['silhouette']
            }
            print(f"   æœ€ä½³: MCS={best['min_cluster_size']}, MS={best['min_samples']} "
                  f"â†’ {best['n_clusters']} clusters, {best['noise_ratio']:.0%} noise")

    # ç¸½çµå»ºè­°
    print(f"\n" + "=" * 80)
    print(f" ğŸ“‹ åƒæ•¸é…ç½®å»ºè­°")
    print(f"=" * 80)

    print("""
    æ ¹æ“šå¯¦é©—çµæœï¼Œå»ºè­°æ¡ç”¨å‹•æ…‹åƒæ•¸ç­–ç•¥ï¼š

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  è³‡æ–™é‡            min_cluster_size    min_samples          â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  < 50 ç¯‡           3                   2                    â”‚
    â”‚  50-100 ç¯‡         5                   3                    â”‚
    â”‚  100-500 ç¯‡        max(5, n*0.05)      max(3, n*0.03)      â”‚
    â”‚  > 500 ç¯‡          max(10, n*0.03)     max(5, n*0.02)      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Noise é–¾å€¼å»ºè­°ï¼ˆCosine Distanceï¼‰ï¼š
    - é«˜ä¿¡å¿ƒåˆ†é…: distance < 0.25
    - æ­£å¸¸åˆ†é…:   distance < 0.35
    - ä½ä¿¡å¿ƒæ¨™è¨˜: distance < 0.45
    - æ¨™è¨˜ Noise: distance >= 0.45
    """)

    # è¼¸å‡º JSON çµæœ
    output = {
        'global_best': ranked_results[0] if ranked_results else None,
        'all_results': ranked_results,
        'topic_results': topic_results,
        'recommendations': {
            'dynamic_params': {
                'small': {'min_cluster_size': 3, 'min_samples': 2, 'threshold': 50},
                'medium': {'min_cluster_size': 5, 'min_samples': 3, 'threshold': 100},
                'large': {'min_cluster_size': 'n*0.05', 'min_samples': 'n*0.03', 'threshold': 500},
            },
            'noise_thresholds': {
                'high_confidence': 0.25,
                'normal': 0.35,
                'low_confidence': 0.45,
                'noise': 0.45
            }
        }
    }

    with open('/Users/adam/poc/ontology/ontix/scripts/cluster_experiment_results.json', 'w') as f:
        json.dump(output, f, indent=2, ensure_ascii=False)

    print(f"\nâœ… çµæœå·²å„²å­˜è‡³ scripts/cluster_experiment_results.json")


if __name__ == '__main__':
    main()
