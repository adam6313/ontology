"""
UMAP é™ç¶­å¯¦é©— - æ¯”è¼ƒä¸åŒç¶­åº¦å°èšé¡å“è³ªçš„å½±éŸ¿

å¯¦é©—ç›®æ¨™ï¼š
1. æ¯”è¼ƒåŸå§‹ç¶­åº¦ vs UMAP é™ç¶­å¾Œçš„èšé¡å“è³ª
2. æ‰¾å‡ºæœ€ä½³é™ç¶­ç›®æ¨™ç¶­åº¦
3. æ¸¬é‡ DBCV, Silhouette, Noise Ratio ä¸‰å€‹æŒ‡æ¨™

åŸ·è¡Œæ–¹å¼ï¼š
    python experiment_umap.py --db-url "postgres://..." --sample-size 1000
"""
import argparse
import time
import json
import numpy as np
import psycopg2
from dataclasses import dataclass
from typing import Optional
from hdbscan import HDBSCAN, validity_index
from umap import UMAP
from sklearn.metrics import silhouette_score
from tabulate import tabulate


@dataclass
class ExperimentResult:
    """å–®æ¬¡å¯¦é©—çµæœ"""
    name: str
    dimensions: int
    n_clusters: int
    noise_ratio: float
    dbcv: float
    silhouette: float
    time_umap: float
    time_hdbscan: float

    @property
    def total_time(self) -> float:
        return self.time_umap + self.time_hdbscan

    @property
    def composite_score(self) -> float:
        """ç¶œåˆåˆ†æ•¸ (è¶Šé«˜è¶Šå¥½)"""
        # DBCV: -1 to 1, higher is better
        # Silhouette: -1 to 1, higher is better
        # Noise ratio: 0 to 1, lower is better
        dbcv_norm = (self.dbcv + 1) / 2  # normalize to 0-1
        sil_norm = (self.silhouette + 1) / 2  # normalize to 0-1
        noise_score = 1 - self.noise_ratio  # invert so higher is better

        # åŠ æ¬Šï¼šDBCV(0.4) + Silhouette(0.3) + NoiseScore(0.3)
        return 0.4 * dbcv_norm + 0.3 * sil_norm + 0.3 * noise_score


def load_embeddings_from_db(db_url: str, limit: int = 1000) -> tuple[np.ndarray, list[str]]:
    """å¾è³‡æ–™åº«è¼‰å…¥ embeddings"""
    print(f"Loading embeddings from database (limit={limit})...")

    conn = psycopg2.connect(db_url)
    cur = conn.cursor()

    cur.execute("""
        SELECT p.post_id, p.content, pe.embedding
        FROM posts p
        JOIN post_embeddings pe ON p.post_id = pe.post_id
        LIMIT %s
    """, (limit,))

    rows = cur.fetchall()
    cur.close()
    conn.close()

    if not rows:
        raise ValueError("No embeddings found in database")

    post_ids = [row[0] for row in rows]
    contents = [row[1] for row in rows]

    # Parse embeddings (stored as vector type)
    embeddings = []
    for row in rows:
        emb = row[2]
        if isinstance(emb, str):
            # Parse "[0.1, 0.2, ...]" format
            emb = json.loads(emb.replace('(', '[').replace(')', ']'))
        embeddings.append(emb)

    embeddings = np.array(embeddings, dtype=np.float32)
    print(f"Loaded {len(embeddings)} embeddings, shape: {embeddings.shape}")

    return embeddings, contents


def load_embeddings_from_file(file_path: str) -> tuple[np.ndarray, list[str]]:
    """å¾ JSON æª”æ¡ˆè¼‰å…¥ embeddings"""
    print(f"Loading embeddings from {file_path}...")

    with open(file_path, 'r') as f:
        data = json.load(f)

    embeddings = np.array([d['embedding'] for d in data], dtype=np.float32)
    contents = [d.get('content', '') for d in data]

    print(f"Loaded {len(embeddings)} embeddings, shape: {embeddings.shape}")
    return embeddings, contents


def run_hdbscan(embeddings: np.ndarray, min_cluster_size: int = 5) -> tuple[np.ndarray, HDBSCAN]:
    """åŸ·è¡Œ HDBSCAN èšé¡"""
    model = HDBSCAN(
        min_cluster_size=min_cluster_size,
        min_samples=max(1, min_cluster_size // 2),
        metric='euclidean',
        cluster_selection_method='eom',
        gen_min_span_tree=True  # éœ€è¦è¨ˆç®— DBCV
    )
    labels = model.fit_predict(embeddings)
    return labels, model


def calculate_metrics(
    embeddings: np.ndarray,
    labels: np.ndarray,
    model: Optional[HDBSCAN] = None
) -> tuple[float, float, float, int]:
    """è¨ˆç®—èšé¡å“è³ªæŒ‡æ¨™"""
    unique_labels = set(labels)
    n_clusters = len([l for l in unique_labels if l >= 0])
    noise_count = sum(1 for l in labels if l == -1)
    noise_ratio = noise_count / len(labels)

    # DBCV (Density-Based Clustering Validation)
    # éœ€è¦è‡³å°‘ 2 å€‹ cluster
    if n_clusters >= 2 and model is not None:
        try:
            dbcv = validity_index(embeddings, labels, metric='euclidean')
        except Exception as e:
            print(f"  DBCV calculation failed: {e}")
            dbcv = -1.0
    else:
        dbcv = -1.0  # ç„¡æ³•è¨ˆç®—

    # Silhouette Score
    # éœ€è¦è‡³å°‘ 2 å€‹ cluster ä¸”æœ‰éå™ªé»
    non_noise_mask = labels >= 0
    if n_clusters >= 2 and sum(non_noise_mask) > n_clusters:
        try:
            silhouette = silhouette_score(
                embeddings[non_noise_mask],
                labels[non_noise_mask]
            )
        except Exception as e:
            print(f"  Silhouette calculation failed: {e}")
            silhouette = -1.0
    else:
        silhouette = -1.0  # ç„¡æ³•è¨ˆç®—

    return dbcv, silhouette, noise_ratio, n_clusters


def run_experiment(
    name: str,
    embeddings: np.ndarray,
    umap_dims: Optional[int],
    min_cluster_size: int = 5
) -> ExperimentResult:
    """åŸ·è¡Œå–®æ¬¡å¯¦é©—"""
    print(f"\n{'='*60}")
    print(f"Experiment: {name}")
    print(f"{'='*60}")

    original_dims = embeddings.shape[1]

    # UMAP é™ç¶­
    if umap_dims is not None and umap_dims < original_dims:
        print(f"Running UMAP: {original_dims} â†’ {umap_dims} dimensions...")

        n_neighbors = min(15, len(embeddings) - 1)

        start = time.time()
        umap_model = UMAP(
            n_neighbors=n_neighbors,
            n_components=umap_dims,
            min_dist=0.0,
            metric='cosine',
            random_state=42
        )
        reduced = umap_model.fit_transform(embeddings)
        time_umap = time.time() - start

        print(f"  UMAP completed in {time_umap:.2f}s")
        final_dims = umap_dims
    else:
        reduced = embeddings
        time_umap = 0.0
        final_dims = original_dims
        print(f"No UMAP (using original {original_dims} dimensions)")

    # HDBSCAN èšé¡
    print(f"Running HDBSCAN (min_cluster_size={min_cluster_size})...")
    start = time.time()
    labels, model = run_hdbscan(reduced, min_cluster_size)
    time_hdbscan = time.time() - start
    print(f"  HDBSCAN completed in {time_hdbscan:.2f}s")

    # è¨ˆç®—æŒ‡æ¨™
    print("Calculating metrics...")
    dbcv, silhouette, noise_ratio, n_clusters = calculate_metrics(reduced, labels, model)

    result = ExperimentResult(
        name=name,
        dimensions=final_dims,
        n_clusters=n_clusters,
        noise_ratio=noise_ratio,
        dbcv=dbcv,
        silhouette=silhouette,
        time_umap=time_umap,
        time_hdbscan=time_hdbscan
    )

    print(f"\nResults:")
    print(f"  Clusters: {n_clusters}")
    print(f"  Noise ratio: {noise_ratio:.1%}")
    print(f"  DBCV: {dbcv:.4f}")
    print(f"  Silhouette: {silhouette:.4f}")
    print(f"  Composite Score: {result.composite_score:.4f}")

    return result


def run_all_experiments(
    embeddings: np.ndarray,
    min_cluster_size: int = 5,
    umap_dimensions: list[int] = None
) -> list[ExperimentResult]:
    """åŸ·è¡Œæ‰€æœ‰å¯¦é©—"""
    original_dims = embeddings.shape[1]

    if umap_dimensions is None:
        # é è¨­æ¸¬è©¦ç¶­åº¦
        umap_dimensions = [10, 20, 30, 50, 100]

    # éæ¿¾æ‰å¤§æ–¼åŸå§‹ç¶­åº¦çš„
    umap_dimensions = [d for d in umap_dimensions if d < original_dims]

    results = []

    # 1. åŸå§‹ç¶­åº¦ (no UMAP)
    result = run_experiment(
        name=f"Original ({original_dims}d)",
        embeddings=embeddings,
        umap_dims=None,
        min_cluster_size=min_cluster_size
    )
    results.append(result)

    # 2. å„ç¨® UMAP ç¶­åº¦
    for dims in umap_dimensions:
        result = run_experiment(
            name=f"UMAP â†’ {dims}d",
            embeddings=embeddings,
            umap_dims=dims,
            min_cluster_size=min_cluster_size
        )
        results.append(result)

    return results


def print_summary(results: list[ExperimentResult]):
    """å°å‡ºå¯¦é©—æ‘˜è¦"""
    print("\n" + "="*80)
    print("EXPERIMENT SUMMARY")
    print("="*80)

    # æº–å‚™è¡¨æ ¼æ•¸æ“š
    headers = [
        "Experiment", "Dims", "Clusters", "Noise%",
        "DBCV", "Silhouette", "Score", "Time(s)"
    ]

    rows = []
    for r in results:
        rows.append([
            r.name,
            r.dimensions,
            r.n_clusters,
            f"{r.noise_ratio:.1%}",
            f"{r.dbcv:.4f}",
            f"{r.silhouette:.4f}",
            f"{r.composite_score:.4f}",
            f"{r.total_time:.2f}"
        ])

    print(tabulate(rows, headers=headers, tablefmt="grid"))

    # æ‰¾å‡ºæœ€ä½³çµæœ
    best = max(results, key=lambda r: r.composite_score)
    print(f"\nğŸ† Best: {best.name} (Score: {best.composite_score:.4f})")

    # æ¯”è¼ƒåŸå§‹ vs æœ€ä½³ UMAP
    original = results[0]
    if best != original:
        score_diff = best.composite_score - original.composite_score
        time_diff = original.total_time - best.total_time
        print(f"\nğŸ“Š Comparison (Original vs Best UMAP):")
        print(f"   Score improvement: {score_diff:+.4f} ({score_diff/original.composite_score*100:+.1f}%)")
        print(f"   Time saved: {time_diff:.2f}s ({time_diff/original.total_time*100:.1f}%)")
        print(f"   DBCV: {original.dbcv:.4f} â†’ {best.dbcv:.4f}")
        print(f"   Silhouette: {original.silhouette:.4f} â†’ {best.silhouette:.4f}")
        print(f"   Noise: {original.noise_ratio:.1%} â†’ {best.noise_ratio:.1%}")


def run_min_cluster_size_experiment(
    embeddings: np.ndarray,
    umap_dims: int = 50,
    sizes: list[int] = None
) -> list[ExperimentResult]:
    """æ¸¬è©¦ä¸åŒ min_cluster_size çš„å½±éŸ¿"""
    if sizes is None:
        sizes = [3, 5, 7, 10, 15, 20]

    print("\n" + "="*80)
    print(f"MIN_CLUSTER_SIZE EXPERIMENT (UMAP â†’ {umap_dims}d)")
    print("="*80)

    # å…ˆåšä¸€æ¬¡ UMAP
    original_dims = embeddings.shape[1]
    n_neighbors = min(15, len(embeddings) - 1)

    print(f"Running UMAP: {original_dims} â†’ {umap_dims} dimensions...")
    umap_model = UMAP(
        n_neighbors=n_neighbors,
        n_components=umap_dims,
        min_dist=0.0,
        metric='cosine',
        random_state=42
    )
    reduced = umap_model.fit_transform(embeddings)

    results = []
    for size in sizes:
        print(f"\n--- min_cluster_size = {size} ---")

        start = time.time()
        labels, model = run_hdbscan(reduced, size)
        time_hdbscan = time.time() - start

        dbcv, silhouette, noise_ratio, n_clusters = calculate_metrics(reduced, labels, model)

        result = ExperimentResult(
            name=f"min_size={size}",
            dimensions=umap_dims,
            n_clusters=n_clusters,
            noise_ratio=noise_ratio,
            dbcv=dbcv,
            silhouette=silhouette,
            time_umap=0,
            time_hdbscan=time_hdbscan
        )
        results.append(result)

        print(f"  Clusters: {n_clusters}, Noise: {noise_ratio:.1%}, Score: {result.composite_score:.4f}")

    return results


def main():
    parser = argparse.ArgumentParser(description='UMAP Dimensionality Reduction Experiment')
    parser.add_argument('--db-url', type=str, help='PostgreSQL connection URL')
    parser.add_argument('--file', type=str, help='JSON file with embeddings')
    parser.add_argument('--sample-size', type=int, default=1000, help='Number of samples to use')
    parser.add_argument('--min-cluster-size', type=int, default=5, help='HDBSCAN min_cluster_size')
    parser.add_argument('--umap-dims', type=str, default='10,20,30,50,100',
                       help='UMAP target dimensions (comma-separated)')
    parser.add_argument('--test-sizes', action='store_true',
                       help='Also test different min_cluster_size values')

    args = parser.parse_args()

    # è¼‰å…¥æ•¸æ“š
    if args.db_url:
        embeddings, contents = load_embeddings_from_db(args.db_url, args.sample_size)
    elif args.file:
        embeddings, contents = load_embeddings_from_file(args.file)
    else:
        # ä½¿ç”¨éš¨æ©Ÿæ•¸æ“šåš demo
        print("No data source specified, using random embeddings for demo...")
        np.random.seed(42)
        embeddings = np.random.randn(500, 128).astype(np.float32)
        contents = [f"Sample text {i}" for i in range(500)]

    # è§£æ UMAP ç¶­åº¦
    umap_dims = [int(d) for d in args.umap_dims.split(',')]

    # åŸ·è¡Œå¯¦é©—
    results = run_all_experiments(
        embeddings,
        min_cluster_size=args.min_cluster_size,
        umap_dimensions=umap_dims
    )

    # å°å‡ºæ‘˜è¦
    print_summary(results)

    # å¯é¸ï¼šæ¸¬è©¦ä¸åŒ min_cluster_size
    if args.test_sizes:
        size_results = run_min_cluster_size_experiment(embeddings, umap_dims=50)
        print("\n" + "="*80)
        print("MIN_CLUSTER_SIZE SUMMARY")
        print("="*80)

        headers = ["min_size", "Clusters", "Noise%", "DBCV", "Silhouette", "Score"]
        rows = [[r.name, r.n_clusters, f"{r.noise_ratio:.1%}",
                 f"{r.dbcv:.4f}", f"{r.silhouette:.4f}", f"{r.composite_score:.4f}"]
                for r in size_results]
        print(tabulate(rows, headers=headers, tablefmt="grid"))

        best_size = max(size_results, key=lambda r: r.composite_score)
        print(f"\nğŸ† Best min_cluster_size: {best_size.name} (Score: {best_size.composite_score:.4f})")

    # è¼¸å‡ºå»ºè­°
    print("\n" + "="*80)
    print("RECOMMENDATIONS")
    print("="*80)

    best = max(results, key=lambda r: r.composite_score)
    original = results[0]

    if best.dimensions < original.dimensions:
        print(f"âœ… UMAP é™ç¶­åˆ° {best.dimensions} ç¶­æ˜¯æœ‰ç›Šçš„")
        print(f"   - èšé¡å“è³ªæå‡ {(best.composite_score - original.composite_score)*100:.1f}%")
        print(f"   - è¨ˆç®—æ™‚é–“æ¸›å°‘ {(original.total_time - best.total_time)/original.total_time*100:.1f}%")
    else:
        print("âŒ UMAP é™ç¶­åœ¨æ­¤æ•¸æ“šé›†ä¸Šæ²’æœ‰æ˜é¡¯å„ªå‹¢")
        print("   å»ºè­°ä¿æŒåŸå§‹ç¶­åº¦é€²è¡Œèšé¡")


if __name__ == '__main__':
    main()
